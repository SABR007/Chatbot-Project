{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d67aee85-f8c9-4281-b9a0-5e9ada39e953",
      "metadata": {
        "id": "d67aee85-f8c9-4281-b9a0-5e9ada39e953"
      },
      "source": [
        "### Import the knowledge graph"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import os\n",
        "os.chdir('/content/drive/My Drive/Speakeasy_Project')"
      ],
      "metadata": {
        "id": "qVhRFOTaxuMs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "590b6834-6aba-40ad-883f-cc8c161513a5"
      },
      "id": "qVhRFOTaxuMs",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "4443faac-5648-49c6-bdd3-896595ccd507",
      "metadata": {
        "id": "4443faac-5648-49c6-bdd3-896595ccd507",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6957e051-f6a9-4d6d-cbca-59ed7549ba36"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rdflib\n",
            "  Downloading rdflib-7.1.1-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting isodate<1.0.0,>=0.7.2 (from rdflib)\n",
            "  Downloading isodate-0.7.2-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: pyparsing<4,>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from rdflib) (3.2.0)\n",
            "Downloading rdflib-7.1.1-py3-none-any.whl (562 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/562.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━\u001b[0m \u001b[32m512.0/562.4 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m562.4/562.4 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading isodate-0.7.2-py3-none-any.whl (22 kB)\n",
            "Installing collected packages: isodate, rdflib\n",
            "Successfully installed isodate-0.7.2 rdflib-7.1.1\n"
          ]
        }
      ],
      "source": [
        "!pip install rdflib # Install the rdflib package\n",
        "from rdflib.term import URIRef, Literal\n",
        "import rdflib\n",
        "import torch\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "id": "4271440a-1527-49de-b840-246f0feeb235",
      "metadata": {
        "id": "4271440a-1527-49de-b840-246f0feeb235",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18305c70-12d7-4ff9-e535-f9cfdc7b1c9b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Graph identifier=N6d6d0318edd14803af89a2803db02046 (<class 'rdflib.graph.Graph'>)>"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ],
      "source": [
        "graph = rdflib.Graph()\n",
        "graph.parse('14_graph.nt', format='turtle')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5518924f-0463-4cb7-ae40-54a2de8cbce6",
      "metadata": {
        "id": "5518924f-0463-4cb7-ae40-54a2de8cbce6"
      },
      "source": [
        "### NameSpaces\n",
        "\n",
        "The entities are stored with different URIs. The most common namespaces are the following:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "id": "d3863b05-3984-4087-8edf-ad588fe73672",
      "metadata": {
        "id": "d3863b05-3984-4087-8edf-ad588fe73672"
      },
      "outputs": [],
      "source": [
        "# define some prefixes\n",
        "WD = rdflib.Namespace('http://www.wikidata.org/entity/')\n",
        "WDT = rdflib.Namespace('http://www.wikidata.org/prop/direct/')\n",
        "DDIS = rdflib.Namespace('http://ddis.ch/atai/')\n",
        "RDFS = rdflib.namespace.RDFS\n",
        "SCHEMA = rdflib.Namespace('http://schema.org/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "id": "8e5c6022-462a-4729-a5ee-0db67861b59c",
      "metadata": {
        "scrolled": true,
        "id": "8e5c6022-462a-4729-a5ee-0db67861b59c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da903856-eaa3-4aea-e748-21f9272705db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Some subjects from the knowledge graph\n",
            "http://www.wikidata.org/entity/Q19637493\n",
            "http://www.wikidata.org/entity/Q28416606\n",
            "http://www.wikidata.org/entity/Q15061318\n",
            "http://www.wikidata.org/entity/Q2986066\n",
            "http://www.wikidata.org/entity/Q545599\n",
            "http://www.wikidata.org/entity/Q30782103\n",
            "http://www.wikidata.org/entity/Q3\n",
            "http://www.wikidata.org/entity/Q1323212\n",
            "http://www.wikidata.org/entity/Q504191\n",
            "http://www.wikidata.org/entity/Q1066948\n",
            "\n",
            " Some objects from the knowledge graph\n",
            "Kemal İnci\n",
            "Edgar Barens\n",
            "http://www.wikidata.org/entity/Q1323212\n",
            "http://www.wikidata.org/entity/Q1066948\n",
            "Eau d'Heure lakes\n",
            "tt1579247\n",
            "tt0072053\n",
            "1972 film by George A. Romero\n",
            "American singer, actor and film producer\n",
            "http://www.wikidata.org/entity/Q3161584\n"
          ]
        }
      ],
      "source": [
        "print('Some subjects from the knowledge graph')\n",
        "for objs in list(set(graph.subjects()))[:10]:\n",
        "    print(objs)\n",
        "\n",
        "print('\\n Some objects from the knowledge graph')\n",
        "for objs in list(set(graph.objects()))[10:20]:\n",
        "    print(objs)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0f12dba1-7ff1-43b8-a66b-b131ed4ca1a5",
      "metadata": {
        "id": "0f12dba1-7ff1-43b8-a66b-b131ed4ca1a5"
      },
      "source": [
        "Some ways to access the label of an entity in the graph subjects given it's URI:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "id": "029c242a-d30f-4255-922e-3a6d90647c54",
      "metadata": {
        "id": "029c242a-d30f-4255-922e-3a6d90647c54",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e53c72d2-08b6-4436-b557-3c6d4816f3ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "node http://www.wikidata.org/entity/Q26705136 has label Juan Andrés Arango\n"
          ]
        }
      ],
      "source": [
        "for node in graph.subjects():\n",
        "    if graph.value(subject=node, predicate=RDFS.label): # Check if the triple exists\n",
        "        print(f\"node {node} has label {graph.value(subject=node, predicate=RDFS.label)}\")\n",
        "    break\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5fc0c3e0-a3c3-4ed2-ac18-724fadaf089a",
      "metadata": {
        "id": "5fc0c3e0-a3c3-4ed2-ac18-724fadaf089a"
      },
      "source": [
        "We want to check if every subject in the graph has a label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "id": "0f5f0fdd-f91f-40db-98b5-765e04c8ff7f",
      "metadata": {
        "id": "0f5f0fdd-f91f-40db-98b5-765e04c8ff7f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9e98d21-2966-40d6-e226-2daf0f7e051c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of subjects with a label: 2051387\n",
            "\n",
            "Number of subjects in the graph: 2056777\n",
            "\n",
            "There are 5390 subject entities without a label\n"
          ]
        }
      ],
      "source": [
        "i = 0\n",
        "j = 0\n",
        "for node in graph.subjects():\n",
        "    j += 1\n",
        "    if graph.value(subject=node, predicate=RDFS.label): # Check if the triple exists\n",
        "        i += 1\n",
        "\n",
        "print(f\"Number of subjects with a label: {i}\\n\")\n",
        "print(f\"Number of subjects in the graph: {j}\\n\")\n",
        "if i != j:\n",
        "    print(f\"There are {j-i} subject entities without a label\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eed26093-bd80-46a4-a55e-4cb5caa5abbe",
      "metadata": {
        "id": "eed26093-bd80-46a4-a55e-4cb5caa5abbe"
      },
      "source": [
        "### Make a dictionary of nodes URIs with the respective labels\n",
        "\n",
        "We want to make a dictionary in which the keys are the nodes URIs and the values are the nodes labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "id": "0aa69a55-b3df-4127-a32a-aaf4396d1ae6",
      "metadata": {
        "id": "0aa69a55-b3df-4127-a32a-aaf4396d1ae6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5821c83-9e58-4c45-cfb2-c7e94eb705a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "URI: http://www.wikidata.org/entity/Q15061318, Label: Aleksandr Chubaryan\n"
          ]
        }
      ],
      "source": [
        "# Function to extract the local part of a URI (e.g., after the last / or #)\n",
        "def extract_label_from_uri(uri, namespaces):\n",
        "    # Loop through all namespaces and remove the matching part\n",
        "    for namespace in namespaces:\n",
        "        if str(uri).startswith(str(namespace)):\n",
        "            return str(uri).replace(str(namespace), \"\")\n",
        "    # If no match, return the original URI\n",
        "    return str(uri).split('/')[-1]\n",
        "\n",
        "# Function to build a dictionary of nodes and their labels\n",
        "def build_node_label_dict(graph, namespaces):\n",
        "    nodes = {}\n",
        "\n",
        "    for node in graph.all_nodes():\n",
        "        if isinstance(node, rdflib.term.URIRef):  # Only process URIs\n",
        "            # Check if the node has a label\n",
        "            label = graph.value(node, RDFS.label)\n",
        "\n",
        "            if label:\n",
        "                # If label exists, use it\n",
        "                nodes[node.toPython()] = str(label)\n",
        "            else:\n",
        "                # If no label, extract the local part of the URI\n",
        "                local_label = extract_label_from_uri(node, namespaces)\n",
        "                nodes[node.toPython()] = local_label\n",
        "\n",
        "    return nodes\n",
        "\n",
        "namespaces = [WD, WDT, DDIS, RDFS, SCHEMA]\n",
        "\n",
        "nodes = build_node_label_dict(graph, namespaces)\n",
        "\n",
        "# Check the result\n",
        "for uri, label in nodes.items():\n",
        "    print(f\"URI: {uri}, Label: {label}\")\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eac4d4e9-79cf-480d-892c-193616839313",
      "metadata": {
        "id": "eac4d4e9-79cf-480d-892c-193616839313"
      },
      "source": [
        "Make an inverse dictionary to find URIs of the entities given the labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "id": "746d40d3-ea9e-4f47-97f3-08d1fc0ffa96",
      "metadata": {
        "id": "746d40d3-ea9e-4f47-97f3-08d1fc0ffa96"
      },
      "outputs": [],
      "source": [
        "ent2uri = {ent: uri for uri, ent in nodes.items()}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b8e8a1bb-a5dd-4258-b1d5-cb9573f1fd87",
      "metadata": {
        "id": "b8e8a1bb-a5dd-4258-b1d5-cb9573f1fd87"
      },
      "source": [
        "We also make another dictionary specifically for predicates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "id": "f0132845-d9aa-4f67-910a-058e76b108d1",
      "metadata": {
        "id": "f0132845-d9aa-4f67-910a-058e76b108d1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7432269f-936f-4e8b-9b29-ab4aa9b91510"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "URI: http://www.wikidata.org/prop/direct/P27, Label: country of citizenship\n"
          ]
        }
      ],
      "source": [
        "# Function to build a dictionary of predicates and their labels\n",
        "def build_pred_label_dict(graph, namespaces):\n",
        "    predicates = {}\n",
        "\n",
        "    for node in graph.predicates():\n",
        "        if isinstance(node, rdflib.term.URIRef):  # Only process URIs\n",
        "            # Check if the node has a label\n",
        "            label = graph.value(node, RDFS.label)\n",
        "\n",
        "            if label:\n",
        "                # If label exists, use it\n",
        "                predicates[node.toPython()] = str(label)\n",
        "\n",
        "            # This condition is never evaluated cause all the predicates have labels\n",
        "            else:\n",
        "                # If no label, extract the local part of the URI\n",
        "                local_label = extract_label_from_uri(node, namespaces)\n",
        "                predicates[node.toPython()] = local_label\n",
        "\n",
        "    return predicates\n",
        "\n",
        "# TODO: change the name of predicates into 'pred2lbl'\n",
        "predicates = build_pred_label_dict(graph, namespaces)\n",
        "\n",
        "# Check the result\n",
        "for uri, label in predicates.items():\n",
        "    print(f\"URI: {uri}, Label: {label}\")\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "16659397-8837-438d-910a-1190c6efbd8e",
      "metadata": {
        "id": "16659397-8837-438d-910a-1190c6efbd8e"
      },
      "source": [
        "Make an inverse dictionary to find URIs of the predicates given the labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "id": "fad9cfe9-044f-4ff1-8213-e9fd11794d59",
      "metadata": {
        "id": "fad9cfe9-044f-4ff1-8213-e9fd11794d59"
      },
      "outputs": [],
      "source": [
        "pred2uri = {pred: uri for uri, pred in predicates.items()}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11364b72-559d-4a6e-8c43-7df59a076234",
      "metadata": {
        "id": "11364b72-559d-4a6e-8c43-7df59a076234"
      },
      "source": [
        "## Embeddings\n",
        "\n",
        "We will now implement an approach that relies on embeddings rather than querying the graph directly. For this we will need to extract entities from the graph in a more dynamic way and will resort to NER"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f2347fc4-9525-4bcc-9656-ea8f8024c0b4",
      "metadata": {
        "id": "f2347fc4-9525-4bcc-9656-ea8f8024c0b4"
      },
      "source": [
        "### NER\n",
        "\n",
        "We choose model 'Babelscape' because it was already trained on a large wikidata dataset and it is by far the best at recognizing movie titles as 'MISC'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "id": "0f8c7a98-afc6-4053-a4bc-433f52f974ae",
      "metadata": {
        "id": "0f8c7a98-afc6-4053-a4bc-433f52f974ae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bdb46027-08a2-4247-951d-8b6e3d087be7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/token_classification.py:170: UserWarning: `grouped_entities` is deprecated and will be removed in version v5.0.0, defaulted to `aggregation_strategy=\"simple\"` instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
        "from transformers import pipeline\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"Babelscape/wikineural-multilingual-ner\")\n",
        "model = AutoModelForTokenClassification.from_pretrained(\"Babelscape/wikineural-multilingual-ner\")\n",
        "\n",
        "ner_pipeline = pipeline(\"ner\", model=model, tokenizer=tokenizer, grouped_entities=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "19a66f7d-193e-4d80-8070-d86a42143c1e",
      "metadata": {
        "id": "19a66f7d-193e-4d80-8070-d86a42143c1e"
      },
      "source": [
        "### Synonyms handling"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3f0c89f5-204e-44b5-a395-3cf556e627d5",
      "metadata": {
        "id": "3f0c89f5-204e-44b5-a395-3cf556e627d5"
      },
      "source": [
        "To account for the presence of synonyms in the question we decided to implement a model that computes the similarity between a phrase and the list of predicates from the knowledge graph and returns the most similar matches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "id": "87629c72-b4f2-432a-a9dd-e36e0bd3d31e",
      "metadata": {
        "id": "87629c72-b4f2-432a-a9dd-e36e0bd3d31e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "773276cf-8692-4637-d282-188cb59bf7a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_md')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "\n",
        "# this command downloads the Spacy model\n",
        "spacy.cli.download(\"en_core_web_md\")\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_md\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "id": "07ce4318-a85e-42ec-b675-696284d04973",
      "metadata": {
        "id": "07ce4318-a85e-42ec-b675-696284d04973"
      },
      "outputs": [],
      "source": [
        "def find_match(phrase, predicate_dict, n=5, confidence=0.6):\n",
        "    \"\"\"\n",
        "    Given a phrase, a dictionary of predicate values, an integer n, and a confidence threshold,\n",
        "    return the top n most similar words to the phrase from the dictionary values\n",
        "    that have a similarity score above the confidence threshold.\n",
        "    \"\"\"\n",
        "    phrase_token = nlp(phrase)\n",
        "    similarities = []\n",
        "\n",
        "    # Calculate similarity between phrase and each predicate value\n",
        "    for predicate in predicate_dict.values():\n",
        "        predicate_token = nlp(predicate)\n",
        "        similarity = phrase_token.similarity(predicate_token)\n",
        "\n",
        "        # Only consider matches above the confidence threshold\n",
        "        if similarity > confidence:\n",
        "            similarities.append((predicate, similarity))\n",
        "\n",
        "    # Sort by similarity in descending order and get the top n matches\n",
        "    top_n_matches = sorted(similarities, key=lambda x: x[1], reverse=True)[:n]\n",
        "\n",
        "    # Return only the most similar words\n",
        "    return [match[0] for match in top_n_matches]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "faec479c-e71c-41c3-b6ca-e430b7c0a482",
      "metadata": {
        "id": "faec479c-e71c-41c3-b6ca-e430b7c0a482"
      },
      "source": [
        "Some weakness of this methods: \"Children\" is not correctly associated to predicate \"Child\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "id": "4633d4e7-45cf-4065-ad8d-584dec6031e2",
      "metadata": {
        "id": "4633d4e7-45cf-4065-ad8d-584dec6031e2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7683b10-9dda-4aa1-8667-d2289a587f0f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['publication date']\n"
          ]
        }
      ],
      "source": [
        "# Example usage\n",
        "phrase = \"release date\"\n",
        "n = 3\n",
        "print(find_match(phrase, predicates, n))  # Should return the top 3 most similar predicates"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e7f687da-cc84-44de-a616-1e85fecfa47b",
      "metadata": {
        "id": "e7f687da-cc84-44de-a616-1e85fecfa47b"
      },
      "source": [
        "### EditDistance Matching\n",
        "\n",
        "For entities the problem of syninyms is not that relevent because generally we can assume that people's names and movie's titles have no synonims. However we still need to make sure that the entities recognized by the NER algorithm correspond to real entities in the knowledge graph, otherwise we cannot map them to an embedding. To achieve this we can use the match_entity function based on editdistance. This function is also useful for predicates."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "id": "62b2ed57-91f4-4f26-a025-9747997efe1b",
      "metadata": {
        "id": "62b2ed57-91f4-4f26-a025-9747997efe1b"
      },
      "outputs": [],
      "source": [
        "import editdistance\n",
        "\n",
        "def match_entity_editdistance(entity, dictionary=nodes, threshold=5):\n",
        "    \"\"\"\n",
        "    Matches the given entity to the closest node in the dictionary based on edit distance.\n",
        "    Returns None if the closest match exceeds the specified distance threshold.\n",
        "\n",
        "    Args:\n",
        "    - entity (str): The entity to match.\n",
        "    - dictionary (dict): The graph dictionary with nodes to match against.\n",
        "    - threshold (int): The maximum allowable edit distance for a match.\n",
        "\n",
        "    Returns:\n",
        "    - (str, str) or (None, None): Returns (node_key, node_value) if a match is found within the threshold,\n",
        "      otherwise returns (None, None).\n",
        "    \"\"\"\n",
        "    tmp = float('inf')  # Start with the highest possible distance\n",
        "    match_node = None\n",
        "    match_value = None\n",
        "\n",
        "    for key, value in dictionary.items():\n",
        "        # Calculate edit distance between the entity and current node value\n",
        "        distance = editdistance.eval(value, entity)\n",
        "        if distance < tmp:\n",
        "            tmp = distance\n",
        "            match_node = key\n",
        "            match_value = value\n",
        "\n",
        "    # Return None if the closest match exceeds the threshold\n",
        "    if tmp > threshold:\n",
        "        return None\n",
        "\n",
        "    return match_node, match_value, tmp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "id": "665cf3c2-1bf4-4ba7-ae24-07d3a153079b",
      "metadata": {
        "id": "665cf3c2-1bf4-4ba7-ae24-07d3a153079b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a94c540b-60e6-41ca-b667-2754bc4a64a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('http://www.wikidata.org/entity/Q25188', 'Inception', 1)\n"
          ]
        }
      ],
      "source": [
        "# Example usage\n",
        "phrase = \"Incption\"\n",
        "print(match_entity_editdistance(phrase, nodes))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "defd30df-100e-4437-893d-7d4875e308c1",
      "metadata": {
        "id": "defd30df-100e-4437-893d-7d4875e308c1"
      },
      "source": [
        "## Extracting Predicates\n",
        "\n",
        "We have implemented the following pipeline to extract predicates from the question:\n",
        "\n",
        "- Extract meaningful words from the question with spacy\n",
        "    - For exaple: from question 'who directed...' only 'directed' is extracted\n",
        "- Generate ngrams from meaningful words\n",
        "    - If the predicate is made of 2 words like \"publication date\" the meaningful word would be ['publication', 'date'] which would not be mapped to 'publication date' but to other words. This is why we generate a list of ngrams like [\"publication 'date\", \"publication\", \"date\"].\n",
        "- Starting with the longest ngram, try to find the predicate from the predicate list that is closest to the ngram. If a match is found, we return it. This means that we prioritize matching longest ngrams\n",
        "    - Before we compare the ngram to the list of predicates we lemmatize it and turn it into a noun using the verb_to_noun dictionary we wrote. This is because many predicates in the list are in the form \"director\", \"writer\" instead of \"direct\" and \"write\"\n",
        "    - We also check if the ngram corresponds exactly to a predicate in the graph via the EditDistance matching function. In tha case the matching predicate is returned immediately\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "id": "38618fcc-6490-42fd-95cb-a0518d4694e9",
      "metadata": {
        "id": "38618fcc-6490-42fd-95cb-a0518d4694e9"
      },
      "outputs": [],
      "source": [
        "verb_to_noun = {\n",
        "    \"affiliate\": \"affiliation\",\n",
        "    \"animate\": \"animator\",\n",
        "    \"base\": \"based on\",\n",
        "    \"cast\": \"cast member\",\n",
        "    \"characterize\": \"characters\",\n",
        "    \"depict\": \"depicts\",\n",
        "    \"describe\": \"node description\",\n",
        "    \"design\": \"designed by\",\n",
        "    \"distribute\": \"distributed by\",\n",
        "    \"educate\": \"educated at\",\n",
        "    \"employ\": \"employer\",\n",
        "    \"found\": \"founded by\",\n",
        "    \"influence\": \"influenced by\",\n",
        "    \"locate\": \"location\",\n",
        "    \"narrate\": \"narrator\",\n",
        "    \"originate\": \"country of origin\",\n",
        "    \"participate\": \"participant in\",\n",
        "    \"perform\": \"performer\",\n",
        "    \"produce\": \"producer\",\n",
        "    \"publish\": \"publication date\",\n",
        "    \"rate\": \"rating\",\n",
        "    \"receive\": \"award received\",\n",
        "    \"represent\": \"represented by\",\n",
        "    \"screen\": \"screenwriter\",\n",
        "    \"study\": \"student of\",\n",
        "    \"write\": \"screenwriter\",\n",
        "    \"direct\": \"director\",\n",
        "    \"photograph\": \"director of photography\",\n",
        "    \"edit\": \"film editor\",\n",
        "    \"speak\": \"languages spoken, written or signed\",\n",
        "    \"produce\": \"production company\",\n",
        "    \"confer\": \"conferred by\",\n",
        "    \"broadcast\": \"broadcast by\",\n",
        "    \"present\": \"presented in\",\n",
        "    \"voice\": \"voice actor\",\n",
        "    \"film\": \"filming location\",\n",
        "    \"release\": \"publication date\",\n",
        "    \"award\": \"award received\",\n",
        "    \"create\": \"creator\",\n",
        "    \"develop\": \"developer\",\n",
        "    \"choreograph\": \"choreographer\",\n",
        "    \"make\": \"production company\",\n",
        "    \"assemble\": \"crew member(s)\",\n",
        "    \"inspire\": \"inspired by\",\n",
        "    \"contribute\": \"contributor to the creative work or subject\",\n",
        "    \"style\": \"costume designer\",\n",
        "    \"nominate\": \"nominated for\",\n",
        "    \"portray\": \"cast member\",\n",
        "    \"describe\": \"node description\",\n",
        "    \"label\": \"node label\",\n",
        "    \"set\": \"narrative location\",\n",
        "    \"shot\": \"filming location\",\n",
        "    \"character\" : \"characters\"\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "id": "94979c4d-d76d-405d-b105-d99637ae9f0c",
      "metadata": {
        "id": "94979c4d-d76d-405d-b105-d99637ae9f0c"
      },
      "outputs": [],
      "source": [
        "# Check if some values in the dict do not correspond to actual entities in the graph\n",
        "for value in verb_to_noun.values():\n",
        "    if value not in predicates.values():\n",
        "        print(f\"{value} to be deleted\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4714f94e-c1ea-41c7-ac4b-a0d5a9bc5878",
      "metadata": {
        "id": "4714f94e-c1ea-41c7-ac4b-a0d5a9bc5878"
      },
      "source": [
        "### Check ngram match\n",
        "\n",
        "We are going to use the check_ngram_match match also to match predicates in the factual question part"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "id": "6924f30a-08c2-4ef2-9e08-db9bf784ea83",
      "metadata": {
        "id": "6924f30a-08c2-4ef2-9e08-db9bf784ea83"
      },
      "outputs": [],
      "source": [
        "def check_ngram_match(ngram, predicate_dict, threshold=2, n=5, confidence=0.6):\n",
        "    \"\"\"\n",
        "    Checks if an n-gram closely matches a predicate in the dictionary.\n",
        "    First, it attempts an exact or close match based on edit distance.\n",
        "    If no close match is found, it falls back to finding the best similarity match.\n",
        "\n",
        "    Args:\n",
        "    - ngram (str): The n-gram to check.\n",
        "    - predicate_dict (dict): Dictionary of known predicates.\n",
        "    - threshold (int): Maximum edit distance for an exact match.\n",
        "    - n (int): Number of top matches to return for similarity matching.\n",
        "    - confidence (float): Minimum similarity threshold for a match.\n",
        "\n",
        "    Returns:\n",
        "    - list: List containing the best-matching predicate or an empty list if no match is found.\n",
        "    \"\"\"\n",
        "    # Check if the ngram matches a predicate exactly or almost exactly\n",
        "    if match_entity_editdistance(ngram, dictionary=predicate_dict, threshold=threshold):\n",
        "        match_node, match_value, _ = match_entity_editdistance(ngram, dictionary=predicate_dict, threshold=threshold)\n",
        "        return [match_value]\n",
        "\n",
        "    # Apply lemmatization before similarity matching\n",
        "    ngram = \" \".join([verb_to_noun.get(token.lemma_, token.lemma_) for token in nlp(ngram)])\n",
        "    matches = find_match(ngram, predicate_dict, n=n, confidence=confidence)\n",
        "\n",
        "    return matches\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "id": "b8736687-1fb3-4e6a-950d-e64b5d84550a",
      "metadata": {
        "id": "b8736687-1fb3-4e6a-950d-e64b5d84550a"
      },
      "outputs": [],
      "source": [
        "def extract_relation_embeddings(sentence, predicate_dict, n=5, confidence=0.6, max_ngram_size=3):\n",
        "    \"\"\"\n",
        "    Extracts the relation from a sentence by finding the most similar predicates,\n",
        "    prioritizing longer n-grams first. If a match with similarity > confidence\n",
        "    is found, it returns that result immediately.\n",
        "\n",
        "    Args:\n",
        "    - sentence (str): The input sentence from which to extract the relation.\n",
        "    - predicate_dict (dict): Dictionary of known predicates with their descriptions.\n",
        "    - n (int): Number of top matches to return.\n",
        "    - confidence (float): Minimum similarity threshold for a match.\n",
        "    - max_ngram_size (int): Maximum number of words in an n-gram to consider for matching.\n",
        "\n",
        "    Returns:\n",
        "    - list: Top `n` predicate matches that have a similarity score above the confidence threshold.\n",
        "    \"\"\"\n",
        "    # Step 1: Parse the sentence to filter stop words and prioritize key phrases\n",
        "    doc = nlp(sentence)\n",
        "    meaningful_words = [token.text for token in doc if not token.is_stop and token.is_alpha]\n",
        "\n",
        "    # Step 2: Generate prioritized n-grams from meaningful words (starting with the longest n-grams)\n",
        "    ngrams = []\n",
        "    for size in range(max_ngram_size, 0, -1):  # Start with larger n-grams\n",
        "        ngrams += [\" \".join(meaningful_words[i:i+size]) for i in range(len(meaningful_words) - size + 1)]\n",
        "\n",
        "    # Step 3: Check each n-gram for similarity, starting with the longest\n",
        "    for ngram in ngrams:\n",
        "        matches = check_ngram_match(ngram, predicate_dict, threshold=2, n=n, confidence=confidence)\n",
        "        if matches:\n",
        "            return matches\n",
        "\n",
        "    # Step 4: If no matches above the confidence threshold are found, return an empty list\n",
        "    return []\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "id": "c32895f7-04da-4f66-984c-d6090b0e0174",
      "metadata": {
        "id": "c32895f7-04da-4f66-984c-d6090b0e0174",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4797f16b-74f7-403b-8707-84c302522f75"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted Relation: ['director']\n"
          ]
        }
      ],
      "source": [
        "sentence = \"Who is the director of ?\"\n",
        "relation = extract_relation_embeddings(sentence, predicates, n=3, confidence=0.5)\n",
        "print(\"Extracted Relation:\", relation)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f18871b8-cd18-4dc8-ab9e-5a873c0d356e",
      "metadata": {
        "id": "f18871b8-cd18-4dc8-ab9e-5a873c0d356e"
      },
      "source": [
        "## Extract embeddings from the files\n",
        "\n",
        "We extract embeddings from the files. We will explain how to use them after the process_question function. Since there is a problem with the relation embeddings we need to extract them now and account for that in the process_question function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "id": "d908c618-8b7a-4ac0-ae5e-24d758a51e52",
      "metadata": {
        "id": "d908c618-8b7a-4ac0-ae5e-24d758a51e52",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        },
        "outputId": "fae3f78d-f13c-44d1-a503-3152ae5b1cd9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"import numpy as np\\nimport csv\\n\\nentity_matrix = np.load('/Users/gianmarcoalbano/Desktop/Advanced topics in AI/Chatbot-Project/ddis-graph-embeddings/entity_embeds.npy')\\npredicate_matrix = np.load('/Users/gianmarcoalbano/Desktop/Advanced topics in AI/Chatbot-Project/ddis-graph-embeddings/relation_embeds.npy')\\n\\nwith open('/Users/gianmarcoalbano/Desktop/Advanced topics in AI/Chatbot-Project/ddis-graph-embeddings/entity_ids.del') as ifile:\\n    ent2id = {ent: int(idx) for idx, ent in csv.reader(ifile, delimiter='\\t')}\\n    id2ent = {v: k for k, v in ent2id.items()}\\nwith open('/Users/gianmarcoalbano/Desktop/Advanced topics in AI/Chatbot-Project/ddis-graph-embeddings/relation_ids.del') as ifile:\\n    pred2id = {rel: int(idx) for idx, rel in csv.reader(ifile, delimiter='\\t')}\\n    id2pred = {v: k for k, v in pred2id.items()}\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 81
        }
      ],
      "source": [
        "'''import numpy as np\n",
        "import csv\n",
        "\n",
        "entity_matrix = np.load('/Users/gianmarcoalbano/Desktop/Advanced topics in AI/Chatbot-Project/ddis-graph-embeddings/entity_embeds.npy')\n",
        "predicate_matrix = np.load('/Users/gianmarcoalbano/Desktop/Advanced topics in AI/Chatbot-Project/ddis-graph-embeddings/relation_embeds.npy')\n",
        "\n",
        "with open('/Users/gianmarcoalbano/Desktop/Advanced topics in AI/Chatbot-Project/ddis-graph-embeddings/entity_ids.del') as ifile:\n",
        "    ent2id = {ent: int(idx) for idx, ent in csv.reader(ifile, delimiter='\\t')}\n",
        "    id2ent = {v: k for k, v in ent2id.items()}\n",
        "with open('/Users/gianmarcoalbano/Desktop/Advanced topics in AI/Chatbot-Project/ddis-graph-embeddings/relation_ids.del') as ifile:\n",
        "    pred2id = {rel: int(idx) for idx, rel in csv.reader(ifile, delimiter='\\t')}\n",
        "    id2pred = {v: k for k, v in pred2id.items()}'''"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Abu Colab Code\n",
        "\n",
        "import numpy as np\n",
        "import csv\n",
        "\n",
        "file_path_embeds = \"/content/drive/My Drive/Speakeasy_Project/ddis-graph-embeddings/entity_embeds.npy\"\n",
        "entity_matrix = np.load(file_path_embeds)\n",
        "\n",
        "file_path_predicate = \"/content/drive/My Drive/Speakeasy_Project/ddis-graph-embeddings/relation_embeds.npy\"\n",
        "predicate_matrix = np.load(file_path_predicate)\n",
        "\n",
        "# Assuming 'entity_ids.del' is in the same directory as other files\n",
        "file_path_entity_ids = \"/content/drive/My Drive/Speakeasy_Project/ddis-graph-embeddings/entity_ids.del\"  # Construct the path for entity_ids.del\n",
        "\n",
        "# Use the constructed path for opening the file\n",
        "with open(file_path_entity_ids) as ifile:  # Changed 'entity_ids.del' to file_path_entity_ids\n",
        "    ent2id = {ent: int(idx) for idx, ent in csv.reader(ifile, delimiter='\\t')}\n",
        "    id2ent = {v: k for k, v in ent2id.items()}\n",
        "\n",
        "# Assuming 'relation_ids.del' is in the same directory as other files\n",
        "file_path_relation_ids = \"/content/drive/My Drive/Speakeasy_Project/ddis-graph-embeddings/relation_ids.del\"  # Construct the path for relation_ids.del\n",
        "\n",
        "# Use the constructed path for opening the file\n",
        "with open(file_path_relation_ids) as ifile:  # Changed 'relation_ids.del' to file_path_relation_ids\n",
        "    pred2id = {rel: int(idx) for idx, rel in csv.reader(ifile, delimiter='\\t')}\n",
        "    id2pred = {v: k for k, v in pred2id.items()}"
      ],
      "metadata": {
        "id": "L2fM3jQyyJKi"
      },
      "id": "L2fM3jQyyJKi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "14ceafa6-7190-4372-bc0a-b30315b2be91",
      "metadata": {
        "id": "14ceafa6-7190-4372-bc0a-b30315b2be91"
      },
      "source": [
        "### Predicates without embeddings\n",
        "\n",
        "There seems to be a problem with the embeddings. Some of them are missing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "id": "462cbdad-4149-4094-b0d6-1ae87cdbfeaa",
      "metadata": {
        "id": "462cbdad-4149-4094-b0d6-1ae87cdbfeaa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53f6c12d-3830-4256-9137-71dadbfe441f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "predicates list: 255\n",
            "relation embeddings list: 248\n",
            "\n",
            "tag has no embedding\n",
            "IMDb ID has no embedding\n",
            "node label has no embedding\n",
            "image has no embedding\n",
            "node description has no embedding\n",
            "publication date has no embedding\n",
            "box office has no embedding\n",
            "rating has no embedding\n"
          ]
        }
      ],
      "source": [
        "print(f\"predicates list: {len(predicates)}\")\n",
        "print(f\"relation embeddings list: {len(predicate_matrix)}\\n\")\n",
        "#pred2id['http://www.wikidata.org/prop/direct/P577']\n",
        "pred_without_embeddings = []\n",
        "# Which predicates are missing an embedding?\n",
        "for predicate in predicates.values():\n",
        "    try:\n",
        "        id = pred2id[pred2uri[predicate]]\n",
        "    except KeyError:\n",
        "        print(f\"{predicate} has no embedding\")\n",
        "        pred_without_embeddings.append(predicate)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a03a67f2-8fbe-422e-ae37-4124c51f4dbd",
      "metadata": {
        "id": "a03a67f2-8fbe-422e-ae37-4124c51f4dbd"
      },
      "source": [
        "# Error handling\n",
        "\n",
        "If our model is unable to find the answer we provide a human like response using the paraphrasing model \"pegasus\". Here is a dimostration of how we plan to use it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "id": "de699fa4-7b78-4cff-afce-566d38491448",
      "metadata": {
        "id": "de699fa4-7b78-4cff-afce-566d38491448",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91408ae6-1044-409c-f9b8-e0372d97da3d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at tuner007/pegasus_paraphrase and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import PegasusForConditionalGeneration, PegasusTokenizer\n",
        "model_name = 'tuner007/pegasus_paraphrase'\n",
        "torch_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
        "model = PegasusForConditionalGeneration.from_pretrained(model_name).to(torch_device)\n",
        "\n",
        "def get_response(input_text,num_return_sequences,num_beams):\n",
        "  batch = tokenizer([input_text],truncation=True,padding='longest',max_length=60, return_tensors=\"pt\").to(torch_device)\n",
        "  translated = model.generate(**batch,max_length=60,num_beams=num_beams, num_return_sequences=num_return_sequences, temperature=1.5)\n",
        "  tgt_text = tokenizer.batch_decode(translated, skip_special_tokens=True)\n",
        "  return tgt_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "id": "cf054d97-7c07-412a-97d4-7487c28bceff",
      "metadata": {
        "id": "cf054d97-7c07-412a-97d4-7487c28bceff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3509ac2c-0745-45e7-f112-f413d67348e5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"I don't know who the main character is in The Masked Gang: Cyprus.\",\n",
              " \"I don't know who the main character of The Masked Gang is.\",\n",
              " \"I don't know who the main character is of The Masked Gang: Cyprus.\"]"
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ],
      "source": [
        "question = \"Who is the main character of The Masked Gang: Cyprus? \"\n",
        "num_beams = 10\n",
        "num_return_sequences = 3\n",
        "context = f\"{question} i don't know\"\n",
        "get_response(context,num_return_sequences,num_beams)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c0854b4-dae3-47d9-82d5-8bbe93e8fd20",
      "metadata": {
        "id": "0c0854b4-dae3-47d9-82d5-8bbe93e8fd20"
      },
      "source": [
        "# Handle multiple questions at once\n",
        "\n",
        "We also implemented a way to split the question into 2 subquestions using spacy, so that we can answer them separately"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "id": "bd92bbd7-256b-4453-bfc8-d0ef88ce83b3",
      "metadata": {
        "id": "bd92bbd7-256b-4453-bfc8-d0ef88ce83b3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5f4823c-7315-4864-f545-44f743a3c702"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Who is the director of Star Wars', 'who is the screenwriter of inception']\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "\n",
        "# Load the spaCy model\n",
        "nlp = spacy.load(\"en_core_web_md\")\n",
        "\n",
        "def split_questions(text):\n",
        "    doc = nlp(text)\n",
        "    questions = []\n",
        "    current_question = []\n",
        "\n",
        "    for token in doc:\n",
        "        # Add token to the current question\n",
        "        current_question.append(token.text)\n",
        "\n",
        "        # If token is a conjunction like 'and', treat it as a potential separator\n",
        "        if token.dep_ == \"cc\" and token.text.lower() == \"and\":\n",
        "            # Join tokens accumulated so far and start a new question\n",
        "            questions.append(\" \".join(current_question[:-1]))\n",
        "            current_question = []\n",
        "\n",
        "    # Add the final question after loop ends\n",
        "    if current_question:\n",
        "        questions.append(\" \".join(current_question))\n",
        "\n",
        "    return [q.strip() for q in questions if q.strip()]\n",
        "\n",
        "# Example compound question\n",
        "question = \"Who is the director of Star Wars and who is the screenwriter of inception\"\n",
        "split_questions_list = split_questions(question)\n",
        "\n",
        "# Result\n",
        "print(split_questions_list)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e2751b60-36e8-423a-a26a-1f1d39f9cef2",
      "metadata": {
        "id": "e2751b60-36e8-423a-a26a-1f1d39f9cef2"
      },
      "source": [
        "# NER Pipeline\n",
        "\n",
        "We can now combine all these functions to successfully extract both predicates and entities from a question. We will use the model \"...\" (we can still decide on a different model) to recognize entities and proceed in the following way:\n",
        "- Preprocess the question so that special characters that hold no important meaning like ! or : are removed\n",
        "- Extract a list of dictionaries with all the entities from the question using NER\n",
        "    - The dictionaries will look like: {'entity_group' : 'PER', 'word': Andrew Garfield'}\n",
        "- Map the extracted entities to actual nodes in the graph via the editdistance function\n",
        "    - If the distance from the entity in the question and the closest entity in the graph is > 5 then no entity is matched\n",
        "    - If the distance from the entity in the question and the closest entity in the graph is < 5 but > 1 then we prompt the chatbot to ask the user to verify if they matched the right enitity\n",
        "- Remove the entities from the question\n",
        "- Pass the question without entities to the predicate_extraction function\n",
        "- Add the extracted predicates to the list of dictionaries as {'entity_group' : 'predicate', 'word': 'screenwriter'}\n",
        "\n",
        "\n",
        "Some notes on how to handle the 'Entity matching too distant' case. In the final notebook with the speakeasy infrastructure you should make a variable with the matched entities that were too distant. so that they are stored for generating the next message in case they answer 'yes' to the question 'did you mean -matched_entity-?'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49825084-47f4-4957-9c09-854579ad3e10",
      "metadata": {
        "id": "49825084-47f4-4957-9c09-854579ad3e10"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
        "from transformers import pipeline\n",
        "import re\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"Babelscape/wikineural-multilingual-ner\")\n",
        "model = AutoModelForTokenClassification.from_pretrained(\"Babelscape/wikineural-multilingual-ner\")\n",
        "\n",
        "ner_pipeline = pipeline(\"ner\", model=model, tokenizer=tokenizer, grouped_entities=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "id": "8bea2def-2c13-48e7-863a-c20ee803e50a",
      "metadata": {
        "id": "8bea2def-2c13-48e7-863a-c20ee803e50a"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def preprocess_question_embeddings(question):\n",
        "    # Remove symbols like :, !, -, etc., by replacing them with an empty string\n",
        "    cleaned_question = re.sub(r'[:!\\\\-]', '', question)\n",
        "    # Remove any extra spaces that might result from removing symbols\n",
        "    cleaned_question = re.sub(r'\\s+', ' ', cleaned_question).strip()\n",
        "    return cleaned_question\n",
        "\n",
        "# Define a function to extract entities and relation from a given question\n",
        "def extract_entities_NER(question, predicate_dict=predicates, n=5, confidence=0.5, max_ngram_size=3):\n",
        "    extracted_entities = []\n",
        "\n",
        "    exit_status = \"\"\n",
        "    question = preprocess_question_embeddings(question)\n",
        "    print(f\"Question after preprocessing: {question}\\n\")\n",
        "\n",
        "    # Step 1: Use the NER pipeline to get entities in the question\n",
        "    entities = ner_pipeline(question)\n",
        "\n",
        "    recommendation_keywords = [\"Recommend\", \"recommend\", \"Recommendation\", \"recommendation\", \"Suggest\", \"suggest\", \"Suggestion\", \"suggestion\", \"like\"]\n",
        "\n",
        "\n",
        "    # If there are no entities in the question return (maybe prompt the user to double check if the capitalized the right letters)\n",
        "    if entities:\n",
        "\n",
        "        # Step 2: Turn dictionaries in entities into simplified dictionaries and concatenate words to join_entity['word']\n",
        "        for entity in entities:\n",
        "            simplified_entity = {\n",
        "                'entity_group': entity['entity_group'],\n",
        "                'word': entity['word']\n",
        "            }\n",
        "            extracted_entities.append(simplified_entity)\n",
        "\n",
        "\n",
        "\n",
        "        # Step 3: Remove extracted entities from the question to isolate the predicate phrase\n",
        "        question_no_entities = question\n",
        "        for entity in extracted_entities:\n",
        "            print(f\"Extracted entity: {entity['word']}\\n\")\n",
        "            # Convert both the question and entity to lowercase for consistent replacement\n",
        "            question_no_entities = re.sub(r'\\b' + re.escape(entity['word'].lower()) + r'\\b', '', question_no_entities.lower(), flags=re.IGNORECASE)\n",
        "\n",
        "        # Replace multiple spaces with a single space and trim leading/trailing whitespace\n",
        "        question_no_entities = re.sub(r'\\s+', ' ', question_no_entities).strip()\n",
        "\n",
        "        print(f\"Question after removing entities: {question_no_entities}\\n\")\n",
        "\n",
        "        # Step 3.5: Match each entity to the closest node in the graph. Remove them if there is no match\n",
        "        for entity in extracted_entities:\n",
        "\n",
        "            if match_entity_editdistance(entity['word'], threshold=5):\n",
        "                match_node, match_value, distance = match_entity_editdistance(entity['word'])\n",
        "\n",
        "                # If the closest entity we can find in the graph is still distant, return the best matched value\n",
        "                # with exit status Entity matching too distant. Then ask the user if the match_value actually\n",
        "                # corresponds to what they wanted\n",
        "                if distance > 5:\n",
        "                    exit_status = 'Entity matching too distant'\n",
        "                    return match_value, exit_status\n",
        "                else:\n",
        "                    # Update 'word' in entity to be the best-matching node's label\n",
        "                    entity['word'] = match_value\n",
        "            else:\n",
        "                # Remove the entity from extracted_entities if no match was found\n",
        "                extracted_entities.remove(entity)\n",
        "\n",
        "    else:\n",
        "        exit_status = 'No entities found by NER'\n",
        "\n",
        "    liked_movies = []\n",
        "    if any(keyword in question for keyword in recommendation_keywords):\n",
        "        #extracted_entities.append({'entity_group': 'recommend_action', 'word': 'recommend'})\n",
        "        liked_movies = [entity['word'] for entity in extracted_entities if entity['entity_group'] == 'MISC']\n",
        "        print(f\"Liked movies: {liked_movies}\\n\")\n",
        "        return liked_movies, exit_status\n",
        "\n",
        "\n",
        "\n",
        "    # Step 4: Extract the relation from the modified question using the extract_relation function\n",
        "    relations = extract_relation_embeddings(question_no_entities if entities else question, predicates, n=n, confidence=confidence, max_ngram_size=max_ngram_size)\n",
        "\n",
        "    # Step 4.5: Check if the extracted relations have an embedding\n",
        "    for relation in relations:\n",
        "        if relation in pred_without_embeddings:\n",
        "            exit_status = 'predicate missing embedding'\n",
        "            return relation, exit_status\n",
        "\n",
        "    # Step 5: Add the relation to the extracted_entities list if a match is found\n",
        "    '''if relations:\n",
        "        print(f\"Extracted predicates: {relations}\\n\")\n",
        "        extracted_entities.append({'entity_group': 'predicate', 'word': []})\n",
        "        for relation in relations:\n",
        "            extracted_entities[-1]['word'].append(relation)'''\n",
        "\n",
        "    if relations:\n",
        "        extracted_entities.append({'entity_group': 'predicate', 'word': []})\n",
        "        for relation in relations:\n",
        "            if relation in pred2uri and pred2uri[relation] in pred2id:  # Check if relation has embedding\n",
        "                extracted_entities[-1]['word'].append(relation)\n",
        "\n",
        "    return extracted_entities, exit_status\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a87816e1-8fa5-4780-90c0-cc0354e8734b",
      "metadata": {
        "id": "a87816e1-8fa5-4780-90c0-cc0354e8734b"
      },
      "outputs": [],
      "source": [
        "sentence = \"Who is the director of The Godfather\"\n",
        "extracted_entities, _ = extract_entities_NER(sentence, predicates, n=2, confidence=0.6)\n",
        "print(\"Extracted entities:\", extracted_entities[0])\n",
        "print(\"Exit status:\", _)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a019e4d0-cce6-4062-bf72-f35e12c32ab3",
      "metadata": {
        "id": "a019e4d0-cce6-4062-bf72-f35e12c32ab3"
      },
      "source": [
        "## Turn labels into Embeddings\n",
        "\n",
        "Now that we have a reliable way of extracting entities and predicates from the question we can turn them into embeddigs:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e9eda7e3-398d-4689-86ca-6cf304f0bcac",
      "metadata": {
        "id": "e9eda7e3-398d-4689-86ca-6cf304f0bcac"
      },
      "source": [
        "ent2id can be used to retrieve the index of an entity in the embedding matrix given it's Uri. Retriving the embedding of an entity given it's label would look like this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "86bc2f81-0296-479a-81fe-78a1d69a1a54",
      "metadata": {
        "scrolled": true,
        "id": "86bc2f81-0296-479a-81fe-78a1d69a1a54"
      },
      "outputs": [],
      "source": [
        "entity_label = 'The Godfather'\n",
        "\n",
        "# Turn label into URI\n",
        "Uri = ent2uri[entity_label]\n",
        "print(f\"The URI of {entity_label} is {Uri}\\n\")\n",
        "\n",
        "# Turn URI into a row index\n",
        "id = ent2id[Uri]\n",
        "print(f\"The id of {entity_label} is {id}\\n\")\n",
        "\n",
        "# Look up the row index in the embedding matrix\n",
        "entity_embedding = entity_matrix[id]\n",
        "print(f\"The embedding of {entity_label} has lenght {len(entity_embedding)}\\n\") # I don't print it cause it's long\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "21633989-8062-46c9-a03a-10cc4bf2d595",
      "metadata": {
        "id": "21633989-8062-46c9-a03a-10cc4bf2d595"
      },
      "outputs": [],
      "source": [
        "entity_label = 'director'\n",
        "\n",
        "# Turn label into URI\n",
        "Uri = pred2uri[entity_label]\n",
        "print(f\"The URI of {entity_label} is {Uri}\\n\")\n",
        "\n",
        "# Turn URI into a row index\n",
        "id = pred2id[Uri]\n",
        "print(f\"The id of {entity_label} is {id}\\n\")\n",
        "\n",
        "# Look up the row index in the embedding matrix\n",
        "entity_embedding = predicate_matrix[id]\n",
        "print(f\"The embedding of {entity_label} has lenght {len(entity_embedding)}\\n\") # I don't print it cause it's long"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "731a787c-5bce-479a-a271-65ebca1bc815",
      "metadata": {
        "id": "731a787c-5bce-479a-a271-65ebca1bc815"
      },
      "source": [
        "### Extract embeddings\n",
        "\n",
        "We write a function to make embedding retrival more straightforward:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca61fa07-eaf2-4826-ae79-d189684e5024",
      "metadata": {
        "id": "ca61fa07-eaf2-4826-ae79-d189684e5024"
      },
      "outputs": [],
      "source": [
        "def extract_embedding(label, type='entity'):\n",
        "\n",
        "    if type=='entity':\n",
        "        pipeline = [ent2uri, ent2id, entity_matrix]\n",
        "    else:\n",
        "        pipeline = [pred2uri, pred2id, predicate_matrix]\n",
        "\n",
        "\n",
        "    Uri = pipeline[0][label]\n",
        "\n",
        "    # Turn URI into a row index\n",
        "    id = pipeline[1][Uri]\n",
        "\n",
        "    # Look up the row index in the embedding matrix\n",
        "    entity_embedding = pipeline[2][id]\n",
        "\n",
        "    return entity_embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3da6cb93-a41b-4d34-9bcd-2e5f73f41bb5",
      "metadata": {
        "id": "3da6cb93-a41b-4d34-9bcd-2e5f73f41bb5"
      },
      "outputs": [],
      "source": [
        "# Example usage\n",
        "\n",
        "entity_label = 'The Godfather'\n",
        "entity_embedding = extract_embedding(entity_label)\n",
        "print(f\"The embedding of {entity_label} has lenght {len(entity_embedding)}\\n\")\n",
        "\n",
        "pred_label = 'director'\n",
        "pred_embedding = extract_embedding(pred_label, 'predicate')\n",
        "print(f\"The embedding of {pred_label} has lenght {len(pred_embedding)}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eb61a295-26fa-4036-9520-e8c087986c2d",
      "metadata": {
        "id": "eb61a295-26fa-4036-9520-e8c087986c2d"
      },
      "source": [
        "### Extract labels\n",
        "\n",
        "We need also a way to turn an embedding into a label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "031643c4-a994-464f-bde5-367a2dcfd8f5",
      "metadata": {
        "id": "031643c4-a994-464f-bde5-367a2dcfd8f5"
      },
      "outputs": [],
      "source": [
        "def extract_label(embedding, type='entity'):\n",
        "\n",
        "    if type=='entity':\n",
        "        pipeline = [entity_matrix, id2ent, nodes]\n",
        "    else:\n",
        "        pipeline = [predicate_matrix, id2pred, predicates]\n",
        "\n",
        "    # Find the index in the entity embeddings matrix that corresponds to the embedding vector\n",
        "    id = np.where((pipeline[0] == embedding).all(axis=1))[0][0]\n",
        "\n",
        "    # Turn the id into a URI\n",
        "    Uri = pipeline[1][id]\n",
        "\n",
        "    # Turn the URI into a label\n",
        "    label = pipeline[2][Uri]\n",
        "\n",
        "    return label\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e944b122-afd5-4c7e-80cd-b2dcbbc4cd4c",
      "metadata": {
        "id": "e944b122-afd5-4c7e-80cd-b2dcbbc4cd4c"
      },
      "outputs": [],
      "source": [
        "# Example usage\n",
        "\n",
        "entity_label = 'The Godfather'\n",
        "entity_embedding = extract_embedding(entity_label)\n",
        "print(f\"The embedding of {entity_label} has lenght {len(entity_embedding)}\\n\")\n",
        "\n",
        "# Turn the embedding back into a label\n",
        "label = extract_label(entity_embedding)\n",
        "print(f\"The extracted label for entity: {entity_label} is {label}\\n\")\n",
        "\n",
        "pred_label = 'characters'\n",
        "pred_embedding = extract_embedding(pred_label, 'predicate')\n",
        "print(f\"The embedding of {pred_label} has lenght {len(pred_embedding)}\\n\")\n",
        "\n",
        "# Turn the embedding back into a label\n",
        "label = extract_label(pred_embedding, 'predicate')\n",
        "print(f\"The extracted label for predicate: {pred_label} is {label}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8addb9fb-1645-4f32-af3d-71967175491e",
      "metadata": {
        "id": "8addb9fb-1645-4f32-af3d-71967175491e"
      },
      "source": [
        "### Evaluate embeddings similarity\n",
        "\n",
        "Given the embedding of an entity we want to find the most similar entities in the graph to said entity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eacf5a57-56f9-4b84-a192-6da08c349738",
      "metadata": {
        "id": "eacf5a57-56f9-4b84-a192-6da08c349738"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import pairwise_distances"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c543acc-793c-4365-9512-229c6f9162cb",
      "metadata": {
        "id": "5c543acc-793c-4365-9512-229c6f9162cb"
      },
      "outputs": [],
      "source": [
        "def find_similarities(embedding, n):\n",
        "\n",
        "    embedding = np.atleast_2d(embedding)\n",
        "\n",
        "    answer = []\n",
        "\n",
        "    dist = pairwise_distances(embedding, entity_matrix)\n",
        "    for idx in dist.argsort().reshape(-1)[:n]:\n",
        "        answer.append(nodes[id2ent[idx]])\n",
        "\n",
        "    return answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cf43a18f-d845-43f4-81f1-f87c1d9764d6",
      "metadata": {
        "scrolled": true,
        "id": "cf43a18f-d845-43f4-81f1-f87c1d9764d6"
      },
      "outputs": [],
      "source": [
        "# Example Usage\n",
        "\n",
        "entity_embedding = extract_embedding('Batman')\n",
        "\n",
        "print(find_similarities(entity_embedding, 5))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##RecSys\n"
      ],
      "metadata": {
        "id": "4SMCn5Iz0aqg"
      },
      "id": "4SMCn5Iz0aqg"
    },
    {
      "cell_type": "markdown",
      "source": [
        "The NER extracts a list of movies that the user likes and appends them to a list. Based on the liked movies, the RecSys will generate recommendations by leveraging both the embeddings of these movies and their genres. First, it checks if each liked movie exists in a knowledge graph. If a movie isn't found, it attempts to find the closest matching entity using the editdistance function. The embeddings for the liked movies (or their closest matches) are then averaged to create a representative vector.\n",
        "\n",
        "Using cosine similarity, it calculates how similar this average embedding is to all other movie embeddings in the graph. It sorts these similarities to identify the most similar movies while excluding any that are already in the question or their closest matches. Finally, it returns a specified number of unique movie recommendations that are not part of the user's liked list."
      ],
      "metadata": {
        "id": "54vKgLLFtlAc"
      },
      "id": "54vKgLLFtlAc"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def recommend_movies_by_genre(liked_movies, n=5):\n",
        "    \"\"\"\n",
        "    Recommends movies similar to the user's liked movies based on embeddings and genre.\n",
        "    Uses closest matching entity if the exact movie is not in the knowledge graph.\n",
        "\n",
        "    Args:\n",
        "        liked_movies (list): List of movie labels liked by the user.\n",
        "        n (int): Number of recommendations to generate.\n",
        "\n",
        "    Returns:\n",
        "        list: List of recommended movie labels.\n",
        "    \"\"\"\n",
        "    liked_movies_embeddings = []\n",
        "    for movie in liked_movies:\n",
        "        #print(f\"Processing movie: {movie}\")\n",
        "        if movie in ent2uri:  # If the movie is in the graph, use its embedding directly\n",
        "            liked_movies_embeddings.append(extract_embedding(movie))\n",
        "            #print(liked_movies_embeddings)\n",
        "\n",
        "\n",
        "        else:  # If not, find the closest matching entity\n",
        "            threshold = max(3, int(len(movie) * 0.9))  # Adjust threshold\n",
        "            match = match_entity_editdistance(movie, threshold=threshold)\n",
        "            if match:\n",
        "                match_node, match_value, _ = match\n",
        "                # Check if the match_value is a valid key before proceeding\n",
        "                if match_value in ent2uri:\n",
        "                    liked_movies_embeddings.append(extract_embedding(match_value))\n",
        "                    print(f\"Using '{match_value}' instead of '{movie}' for recommendation.\")\n",
        "                else:\n",
        "                    print(f\"Closest match '{match_value}' not found in embedding data. Skipping.\")\n",
        "            else:\n",
        "                print(f\"Could not find a close match for '{movie}' in the knowledge graph.\")\n",
        "\n",
        "    if not liked_movies_embeddings:\n",
        "        return \"None of the provided movies were found in the knowledge graph.\"\n",
        "\n",
        "    # Calculate the average embedding of liked movies (or their closest matches)\n",
        "    #print(f\"Liked movies embeddings: {liked_movies_embeddings}\")\n",
        "    avg_embedding = np.mean(liked_movies_embeddings, axis=0)\n",
        "\n",
        "    # Calculate similarity to all other movie embeddings\n",
        "    similarities = cosine_similarity(avg_embedding.reshape(1, -1), entity_matrix)\n",
        "\n",
        "    # Get indices of most similar movies (excluding liked movies and their close matches)\n",
        "    sorted_indices = similarities.argsort()[0][::-1]  # Sort in descending order\n",
        "\n",
        "    excluded_movies = set(liked_movies + [match_value for _, match_value, _ in [match_entity_editdistance(m) for m in liked_movies if match_entity_editdistance(m)]if match_value])\n",
        "\n",
        "    # Filter recommended indices, excluding liked movies and their close matches\n",
        "    # The change is here: Add a check if id2ent[i] is in nodes before accessing it\n",
        "    recommended_indices = [i for i in sorted_indices if id2ent[i] in nodes and nodes[id2ent[i]] not in excluded_movies]\n",
        "\n",
        "    # Return labels of recommended movies\n",
        "    recommendations = [nodes[id2ent[i]] for i in recommended_indices[:n]]\n",
        "    return recommendations"
      ],
      "metadata": {
        "id": "H88pbE3w0dok"
      },
      "id": "H88pbE3w0dok",
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "0e9625da-eeb8-4b7c-8394-0360f83a77e8",
      "metadata": {
        "id": "0e9625da-eeb8-4b7c-8394-0360f83a77e8"
      },
      "source": [
        "# Answer questions with Embeddings\n",
        "\n",
        "We can now use the following pipeline for answering questions:\n",
        "- Extract the entities and relation from the question\n",
        "- Turn entities and relation into embeddings\n",
        "- If the entity is a subject, retrieve the object by: _object = subject + relation_\n",
        "- If the entity is an object, retrieve the subject by _subject = object - relation_"
      ]
    },
    {
      "source": [
        "def answer_question_embeddings(question):\n",
        "    entities, exit_status = extract_entities_NER(question, predicates, n=3, confidence=0.6)\n",
        "    recommendation_keywords = [\"Recommend\", \"recommend\", \"Recommendation\", \"recommendation\", \"Suggest\", \"suggest\", \"Suggestion\", \"suggestion\", \"like\"]\n",
        "    liked_movies = []\n",
        "\n",
        "    # Handle cases based on exit_status\n",
        "    if exit_status == 'No entities found by NER':\n",
        "        return \"We could not find any entities in the question. Could you verify that you have capitalized the right letters, such as movie titles or people’s names?\"\n",
        "\n",
        "    elif exit_status == 'Entity matching too distant':\n",
        "        #match_value, _ = entities  # entities contains the match value in this case\n",
        "        #return f\"The closest entity match found was '{match_value}', but it seems too distant. Could you rephrase it or specify it more clearly?\"\n",
        "        # Check if entities is a tuple with more than 2 elements\n",
        "        if isinstance(entities, tuple) and len(entities) > 2:\n",
        "            # If so, assume the first element is the match value\n",
        "            match_value = entities[0]\n",
        "        else:\n",
        "            # Otherwise, unpack as before if there are only two elements\n",
        "            try:\n",
        "                match_value, _ = entities\n",
        "            except (TypeError, ValueError):\n",
        "                # Handle cases where entities is not iterable or has unexpected format\n",
        "                return \"Error: Unexpected format for entities. Please check the extract_entities_NER function.\"\n",
        "        return f\"The closest entity match found was '{match_value}', but it seems too distant. Could you rephrase it or specify it more clearly?\"\n",
        "\n",
        "\n",
        "    elif exit_status == 'predicate missing embedding':\n",
        "        relation, _ = entities  # entities contains the relation in this case\n",
        "        return f\"Unfortunately, we were not provided with an embedding for the relation '{relation}'. Please try another question.\"\n",
        "\n",
        "    elif any(keyword in question for keyword in recommendation_keywords):\n",
        "        # Assuming extract_entities_NER returns a tuple of (match_value, liked_movies)\n",
        "        # for recommendation-type queries\n",
        "        liked_movies = entities\n",
        "        # If entities contains more than two elements, assume the liked movies\n",
        "        # are stored in the second element\n",
        "\n",
        "        return recommend_movies_by_genre(liked_movies)\n",
        "    # Proceed if everything worked correctly\n",
        "    # Check if entities is a list of dictionaries before proceeding\n",
        "    if not exit_status and isinstance(entities, list) and all(isinstance(item, dict) for item in entities):\n",
        "        extracted_predicates = [d['word'] for d in entities if d['entity_group'] == 'predicate']\n",
        "\n",
        "        # Check if extracted_predicates is empty\n",
        "        if extracted_predicates:\n",
        "            extracted_predicates = extracted_predicates[0]  # Access the first element only if it exists\n",
        "        else:\n",
        "            # Handle the case where no predicates are found (e.g., return an error message or a default value)\n",
        "            return \"No predicate found in the question.\"  # Or handle it differently\n",
        "\n",
        "        extracted_entities = [d['word'] for d in entities if d['entity_group'] != 'predicate']\n",
        "        # Extract predicates and entities\n",
        "        #extracted_predicates = [d['word'] for d in entities if d['entity_group'] == 'predicate'][0]\n",
        "        #extracted_entities = [d['word'] for d in entities if d['entity_group'] != 'predicate']\n",
        "\n",
        "        # Convert predicates and entities to embeddings\n",
        "        predicates_embeddings = [extract_embedding(pred, 'predicate') for pred in extracted_predicates]\n",
        "        entities_embeddings = [extract_embedding(ent) for ent in extracted_entities]\n",
        "\n",
        "        # Compute answer using similarity function\n",
        "        answer = find_similarities(entities_embeddings[0] + predicates_embeddings[0], 3)\n",
        "        return answer\n",
        "    else:\n",
        "        return \"Error: Invalid format for entities. Please check the extract_entities_NER function.\""
      ],
      "cell_type": "code",
      "metadata": {
        "id": "thiqndfUBsl9"
      },
      "id": "thiqndfUBsl9",
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "id": "1251c5d7-4008-429e-a518-3a4ef51645b2",
      "metadata": {
        "id": "1251c5d7-4008-429e-a518-3a4ef51645b2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "517cf9e5-2f9e-495b-fd10-d3ab025b20d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question after preprocessing: Who is the director of Star Wars Episode VI Return of the Jedi?\n",
            "\n",
            "Extracted entity: Star Wars Episode VI Return of the Jedi\n",
            "\n",
            "Question after removing entities: who is the director of ?\n",
            "\n",
            "['George Lucas', 'Anthony Daniels', 'Ellis Rubin']\n"
          ]
        }
      ],
      "source": [
        "# Example Usage\n",
        "\n",
        "question = \"Who is the director of Star Wars: Episode VI - Return of the Jedi?\"\n",
        "\n",
        "print(answer_question_embeddings(question))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question1 = \"Given that I like Inception and Jumanji, can you recommend some movies?\"\n",
        "\n",
        "print(answer_question_embeddings(question1))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6cde161-5d2c-4160-fb1c-5674e6e130a4",
        "id": "rmoMWAz4sg-A"
      },
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question after preprocessing: Given that I like Inception and Jumanji, can you recommend some movies?\n",
            "\n",
            "Extracted entity: Inception\n",
            "\n",
            "Extracted entity: Jumanji\n",
            "\n",
            "Question after removing entities: given that i like and , can you recommend some movies?\n",
            "\n",
            "Liked movies: ['Inception', 'Jumanji']\n",
            "\n",
            "['Bee Movie', 'The Wolverine', 'The Little Mermaid', 'Through the Wormhole', 'The Rocketeer']\n"
          ]
        }
      ],
      "id": "rmoMWAz4sg-A"
    },
    {
      "cell_type": "code",
      "source": [
        "question2 = \"Recommend movies like Nightmare on Elm Street, Friday the 13th, and Halloween.\"\n",
        "\n",
        "print(answer_question_embeddings(question2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "405e97e3-ef1d-4a50-d06d-5d4c79416126",
        "id": "SOzggo3isg-G"
      },
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question after preprocessing: Recommend movies like Nightmare on Elm Street, Friday the 13th, and Halloween.\n",
            "\n",
            "Extracted entity: Nightmare on Elm Street\n",
            "\n",
            "Extracted entity: Friday the 13th\n",
            "\n",
            "Extracted entity: Halloween\n",
            "\n",
            "Question after removing entities: recommend movies like , , and .\n",
            "\n",
            "Liked movies: ['A Nightmare on Elm Street', 'Friday the 13th', 'Halloween']\n",
            "\n",
            "['A Nightmare on Elm Street 4: The Dream Master', 'Final Destination', 'Jason X', 'A Nightmare on Elm Street 5: The Dream Child', 'A Nightmare on Elm Street 3: Dream Warriors']\n"
          ]
        }
      ],
      "id": "SOzggo3isg-G"
    },
    {
      "cell_type": "code",
      "source": [
        "question3 = \"Given that I like Madagascar 1, Pocahontas, and Rio, can you recommend some movies? \"\n",
        "\n",
        "print(answer_question_embeddings(question3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a79024c-fb0c-4c7a-e6d2-351dd926fa85",
        "id": "HdAHsZ-Csg-H"
      },
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question after preprocessing: Given that I like Madagascar 1, Pocahontas, and Rio, can you recommend some movies?\n",
            "\n",
            "Extracted entity: Madagascar 1\n",
            "\n",
            "Extracted entity: Pocahontas\n",
            "\n",
            "Extracted entity: Rio\n",
            "\n",
            "Question after removing entities: given that i like , , and , can you recommend some movies?\n",
            "\n",
            "Liked movies: ['Madagascar', 'Pocahontas', 'Rio']\n",
            "\n",
            "['Robots', 'Home', 'Penguins of Madagascar', 'Shrek Forever After', \"Madagascar 3: Europe's Most Wanted\"]\n"
          ]
        }
      ],
      "id": "HdAHsZ-Csg-H"
    },
    {
      "cell_type": "code",
      "source": [
        "question6 = \"Given that I like The Lion King, Pocahontas, and The Beauty and the Beast, can you recommend some movies?\"\n",
        "\n",
        "print(answer_question_embeddings(question6))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e28fd99-7663-4f76-c035-86f36d485707",
        "id": "q9W-aVbDsg-H"
      },
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question after preprocessing: Given that I like The Lion King, Pocahontas, and The Beauty and the Beast, can you recommend some movies?\n",
            "\n",
            "Extracted entity: The Lion King\n",
            "\n",
            "Extracted entity: Pocahontas\n",
            "\n",
            "Extracted entity: The Beauty and the Beast\n",
            "\n",
            "Question after removing entities: given that i like , , and , can you recommend some movies?\n",
            "\n",
            "Liked movies: ['The Lion King', 'Pocahontas', 'Beauty and the Beast']\n",
            "\n",
            "[\"The Lion King II: Simba's Pride\", 'The Lion Guard', 'The King', 'The Little Mermaid', 'The Great Mouse Detective']\n"
          ]
        }
      ],
      "id": "q9W-aVbDsg-H"
    },
    {
      "cell_type": "markdown",
      "id": "cfd239b3-664e-4461-9dd9-940b4bb3f463",
      "metadata": {
        "id": "cfd239b3-664e-4461-9dd9-940b4bb3f463"
      },
      "source": [
        "# TODO\n",
        "\n",
        "- Implement way to handle double questions like \"who is the director of ... AND who is the screenwriter of ....\"\n",
        "- Finish and perfect factual questions queries\n",
        "- Implement language model to generate more realistic responses"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7c1537d3-888a-48ea-8c0b-b373fccde4d2",
      "metadata": {
        "id": "7c1537d3-888a-48ea-8c0b-b373fccde4d2"
      },
      "source": [
        "## Factual question answering\n",
        "\n",
        "For factual question we will proceed in the following way:\n",
        "- Write a list of common question patterns using re to extract relations and entities from them\n",
        "- Extract the probable relations and entities from the question matching it to the pattern and map those probable entities to actual entities in the graph using edit distance or embedding similarity with spacy\n",
        "- Generate a custom sparql query based on the question pattern matched\n",
        "- Query the graph with the custom query"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e0c60ef5-cb24-4560-a599-31506aa8041a",
      "metadata": {
        "id": "e0c60ef5-cb24-4560-a599-31506aa8041a"
      },
      "source": [
        "### Step 1: Write a list of question patterns\n",
        "The question pattern contains:\n",
        "- The actual pattern like r\"(?:find|which) movies.*contain(?:s)?(?: the word)? (?P<word>\\w+)\"\n",
        "- A string associated to that specific question type which we are going to use to map question type to custom queries\n",
        "- A boolean value that is 1 if we want to match the entities from the question to actual entities in the graph and 0 if we don't. One example could be question type: r\"movies rated below (?P<number>\\d+(\\.\\d+)?)\" which asks the chatbot to list all the movies rated below a certain score. In this case there are no entities to be retrieved from the graph\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d9435373-5dcd-4337-9772-e92cb0d73e15",
      "metadata": {
        "id": "d9435373-5dcd-4337-9772-e92cb0d73e15"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "question_patterns = [\n",
        "\n",
        "    # Pattern 0: who and what\n",
        "    (r\"who is the (?P<relation>.+?) of (?P<entity>.+)\", 'who', 1),\n",
        "\n",
        "    # Pattern 1: Find movies with (word) in their titles\n",
        "    (r\"(?:find|which) movies.*contain(?:s)?(?: the word)? (?P<word>\\w+)\", 'find_word_in_title', 0),\n",
        "    (r\"(?:find|which) movies with (?P<word>\\w+) in (?:their )?titles?\", 'find_word_in_title', 0),\n",
        "    (r\"(?:find|which) movies (?:whose )?(?:title|name) contains? (?P<word>\\w+)\", 'find_word_in_title', 0),\n",
        "\n",
        "    # Pattern 2: Highest-rated movies (optional 'above' and a number)\n",
        "    (r\"(?:what are|list)(?: the)?(?: highest[-\\s]rated)? movies(?: rated)?(?: above| greater than)?(?: (?P<number>\\d+(\\.\\d+)?))?\", 'movies_rating_above', 0),\n",
        "    (r\"movies (?:rated )?(?:above )?(?P<number>\\d+(\\.\\d+)?)?\", 'movies_rating_above', 0),\n",
        "\n",
        "    # Pattern 3: Lowest-rated movies (optional 'below' and a number)\n",
        "    (r\"(?:what are|list)(?: the)?(?: lowest[-\\s]rated)? movies(?: rated)?(?: below| less than)?(?: (?P<number>\\d+(\\.\\d+)?))?\", 'movies_rating_below', 0),\n",
        "    (r\"movies (?:rated )?(?:below )?(?P<number>\\d+(\\.\\d+)?)?\", 'movies_rating_below', 0),\n",
        "\n",
        "    # Pattern 4: Entities in alphabetical order\n",
        "    (r\"which (?P<entity>.+) comes first alphabetically\", 'entity_first_alphabetically', 1),\n",
        "    (r\"list (?P<entity>.+) in alphabetical order\", 'entity_first_alphabetically', 1),\n",
        "\n",
        "    # Pattern 5: Entities in reverse alphabetical order\n",
        "    (r\"which (?P<entity>.+) comes last alphabetically\", 'entity_last_alphabetically', 1),\n",
        "    (r\"list (?P<entity>.+) in reverse alphabetical order\", 'entity_last_alphabetically', 1),\n",
        "\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2d6ce000-8648-48d9-a541-65dfe2011b24",
      "metadata": {
        "id": "2d6ce000-8648-48d9-a541-65dfe2011b24"
      },
      "source": [
        "### Step 2: Process question to extract entities\n",
        "\n",
        "We will proceed in the following way:\n",
        "\n",
        "- Extract the dictionary of matched entities via the .groupdict() method.\n",
        "    - For question r\"who is the director of Star Wars\" matched to pattern r\"who is the (?P<relation>.+?) of (?P<entity>.+)\" the dictionary looks like this {'relation': 'director', 'entity': 'Star Wars'}\n",
        "- Append the question type to the dictionary. So for r\"who is the director of Star Wars\" it would result in: {'relation': 'director', 'entity': 'Star Wars', 'qtype': 'who'}\n",
        "- If the question type needs to match entities to the knowledge graph (boolean value == 1) then we gather the relation and/or entity that we extracted from the question via matching the pattern and we match those values to actual entities in the graph\n",
        "    - For entities we try to match them with the match_entity_editdistance function.\n",
        "    - For predicates we need to account for synonyms so we use the check_ngram_match function that takes a potential predicate and tries to match it via editDistance to an actual predicate and if it doesn't work it tries by evaluating embeddings similarity via spacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4cc83579-8238-4751-9d25-0c537c7438f7",
      "metadata": {
        "id": "4cc83579-8238-4751-9d25-0c537c7438f7"
      },
      "outputs": [],
      "source": [
        "def process_question_factual(question, entity_dictionary, predicate_dictionary):\n",
        "\n",
        "    for pattern, qtype, matching in question_patterns:\n",
        "\n",
        "        print(f\"pattern = {pattern}\")\n",
        "        match = re.match(pattern, question, re.IGNORECASE)\n",
        "\n",
        "        if match:\n",
        "            params = match.groupdict()\n",
        "            params['type'] = qtype  # Add the question type to the params\n",
        "            print(f\"Question matched to pattern {qtype}\\n\")\n",
        "\n",
        "            if matching:\n",
        "                # Extract and match the relation and entity\n",
        "                relation = params.get('relation', \"\").lower()  # Set default as empty string\n",
        "                entity = params.get('entity', \"\") # Set default as empty string (don't lower it)\n",
        "\n",
        "                # Match the entity to the closest in the knowledge graph. Returns an uri and label of the closest entity, and the distance\n",
        "                _, matched_entity_label, _ = match_entity_editdistance(entity, dictionary=entity_dictionary) if entity else None\n",
        "\n",
        "                # Match the relation to the closest in the knowledge graph. check_ngram_match returns a list\n",
        "                matched_predicate_label = check_ngram_match(relation, predicates, threshold=2, n=5, confidence=0.6) if relation else None\n",
        "\n",
        "                # Update entity and predicate with the matched labels\n",
        "                if matched_entity_label:\n",
        "                    params['entity'] = matched_entity_label\n",
        "                if matched_predicate_label:\n",
        "                    params['relation'] = matched_predicate_label[0]\n",
        "\n",
        "            return params\n",
        "\n",
        "    return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c60d0cf-936f-47f6-92a9-02cbd2eb0428",
      "metadata": {
        "scrolled": true,
        "id": "7c60d0cf-936f-47f6-92a9-02cbd2eb0428"
      },
      "outputs": [],
      "source": [
        "# Example usage\n",
        "\n",
        "user_input = {\n",
        "    0: \"Who is the director of Star Wars\",\n",
        "    1: \"Which movies whose name contains italy\",\n",
        "    2: \"List movies rated above 7\",\n",
        "    3: \"what are the lowest-rated movies?\",\n",
        "    4: \"Which films comes first alphabetically\",\n",
        "    5: \"list actors in reverse alphabetical order\"\n",
        "}\n",
        "\n",
        "for pattern, question in user_input.items():\n",
        "\n",
        "    params = process_question_factual(question, nodes, predicates)\n",
        "\n",
        "    if params:\n",
        "        print(f\"Pattern {pattern}: Question: {question}\\n\")\n",
        "\n",
        "        for key, value in params.items():\n",
        "            print(f\"{key} : {params[key]}\\n\")\n",
        "        print(\"\\n\\n\")\n",
        "\n",
        "    else:\n",
        "        print(f\"\\nPattern {pattern} not matched\\n\\n\\n\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "58ab23ca-a9a9-4ff4-a254-522dd606833f",
      "metadata": {
        "id": "58ab23ca-a9a9-4ff4-a254-522dd606833f"
      },
      "source": [
        "### Step 3: Match question types to SPARQL queries\n",
        "\n",
        "The generate_sparql_query function takes the type of the question found in the params dictionary and generates the correspnding query. For example for question \"Who is the director of Star Wars\" it generates a query that query the graph looking for an entity that has relation \"director\" with the entity \"Star Wars\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f06ee64f-ec3f-4ccf-ac3a-90bed2344753",
      "metadata": {
        "id": "f06ee64f-ec3f-4ccf-ac3a-90bed2344753"
      },
      "outputs": [],
      "source": [
        "def generate_sparql_query(params):\n",
        "    qtype = params.get('type')\n",
        "\n",
        "    if qtype == 'who':\n",
        "        sparql_query = f\"\"\"\n",
        "        SELECT ?result WHERE {{\n",
        "            ?entity rdfs:label \"{params['entity']}\"@en .\n",
        "            ?entity <{pred2uri[params['relation']]}> ?item .\n",
        "            ?item rdfs:label ?result .\n",
        "            FILTER (lang(?result) = 'en')\n",
        "        }}\n",
        "        \"\"\"\n",
        "        return sparql_query\n",
        "\n",
        "\n",
        "    # Fix: this query returns names of all the entities whose label contains the word, not just movies\n",
        "    elif qtype == 'find_word_in_title':\n",
        "        word = params.get('word')\n",
        "        sparql_query = f\"\"\"\n",
        "        SELECT ?movieLabel WHERE {{\n",
        "            ?movie rdfs:label ?movieLabel .\n",
        "            FILTER(CONTAINS(LCASE(?movieLabel), LCASE(\"{word}\"))) .\n",
        "            FILTER (lang(?movieLabel) = 'en')\n",
        "        }}\n",
        "        \"\"\"\n",
        "        return sparql_query\n",
        "\n",
        "    elif qtype == 'movies_rating_above':\n",
        "        number = params.get('number')\n",
        "        sparql_query = f\"\"\"\n",
        "        SELECT ?movieLabel WHERE {{\n",
        "            ?movie ddis:rating ?rating .\n",
        "            FILTER(?rating > {number}) .\n",
        "            ?movie rdfs:label ?movieLabel .\n",
        "            FILTER (lang(?movieLabel) = 'en')\n",
        "        }} ORDER BY DESC(?rating) LIMIT 1\n",
        "        \"\"\"\n",
        "        return sparql_query\n",
        "\n",
        "    elif qtype == 'movies_rating_below':\n",
        "        number = params.get('number')\n",
        "        sparql_query = f\"\"\"\n",
        "        SELECT ?movieLabel WHERE {{\n",
        "            ?movie ddis:rating ?rating .\n",
        "            FILTER(?rating < {number}) .\n",
        "            ?movie rdfs:label ?movieLabel .\n",
        "            FILTER (lang(?movieLabel) = 'en')\n",
        "        }} ORDER BY DESC(?rating)\n",
        "        \"\"\"\n",
        "        return sparql_query\n",
        "\n",
        "    elif qtype == 'entity_first_alphabetically':\n",
        "        sparql_query = f\"\"\"\n",
        "        SELECT ?entity_label WHERE {{\n",
        "            ?entity wdt:P31 <{params['matched_entity_uri']}> .\n",
        "            ?entity rdfs:label ?entity_label .\n",
        "            FILTER (lang(?entity_label) = 'en')\n",
        "        }} ORDER BY ASC(?entity_label)\n",
        "        \"\"\"\n",
        "        return sparql_query\n",
        "\n",
        "    elif qtype == 'entity_last_alphabetically':\n",
        "        sparql_query = f\"\"\"\n",
        "        SELECT ?entity_label WHERE {{\n",
        "            ?entity wdt:P31 <{params['matched_entity_uri']}> .\n",
        "            ?entity rdfs:label ?entity_label .\n",
        "            FILTER (lang(?entity_label) = 'en')\n",
        "        }} ORDER BY DESC(?entity_label)\n",
        "        \"\"\"\n",
        "        return sparql_query\n",
        "\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "'''\n",
        "TODO\n",
        "you shoud include prefixes in each query!\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "id": "3470a349-2778-497f-baf4-c42997a1075c",
      "metadata": {
        "id": "3470a349-2778-497f-baf4-c42997a1075c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83402e60-f931-4b98-e8df-a5ad6b6c03ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Star Wars']\n"
          ]
        }
      ],
      "source": [
        "print(check_ngram_match('Star Wars', nodes))\n",
        "\n",
        "sparql_query = f\"\"\"\n",
        "        SELECT ?result WHERE {{\n",
        "            ?entity rdfs:label \"{check_ngram_match('Star Wars', nodes)[0]}\"@en .\n",
        "            ?entity <{pred2uri['director']}> ?item .\n",
        "            ?item rdfs:label ?result .\n",
        "            FILTER (lang(?result) = 'en')\n",
        "        }}\n",
        "        \"\"\"\n",
        "for res in graph.query(sparql_query):\n",
        "    print(res)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4aaab09c-835c-48bc-bb72-ea369dd49867",
      "metadata": {
        "id": "4aaab09c-835c-48bc-bb72-ea369dd49867"
      },
      "source": [
        "### Step 4: Query the graph\n",
        "\n",
        "We implement a function that takes the graph and query and returns the result and an exit code that is an empty string if results were found, and \"No results\" if no results were found"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "id": "f8257008-8701-4d28-be80-7a219e00fcc0",
      "metadata": {
        "id": "f8257008-8701-4d28-be80-7a219e00fcc0"
      },
      "outputs": [],
      "source": [
        "def query_graph(graph, sparql_query):\n",
        "\n",
        "    # Execute the query\n",
        "    qres = graph.query(sparql_query)\n",
        "\n",
        "    # Process the results\n",
        "    results = []\n",
        "    for row in qres:\n",
        "        results.append(str(row.result))\n",
        "\n",
        "    # Check if we have results, if not return exit_code = \"No results\"\n",
        "    return results"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "143e8988-f5cc-4f00-92a0-2e37bf67821d",
      "metadata": {
        "id": "143e8988-f5cc-4f00-92a0-2e37bf67821d"
      },
      "source": [
        "### Step 5: Put everything together\n",
        "\n",
        "We implement a function that takes a question and computes the factual answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "id": "acf11be8-3cef-488e-a917-6d065100b4f7",
      "metadata": {
        "id": "acf11be8-3cef-488e-a917-6d065100b4f7"
      },
      "outputs": [],
      "source": [
        "def answer_question_factual(question):\n",
        "\n",
        "    if process_question_factual(question, nodes, predicates):\n",
        "        params = process_question_factual(question, nodes, predicates)\n",
        "        print(f\"Parameters found: {params}\\n\")\n",
        "    else:\n",
        "        return exit_message\n",
        "\n",
        "    sparql_query = generate_sparql_query(params)\n",
        "\n",
        "    answer = query_graph(graph, sparql_query)\n",
        "\n",
        "    if answer:\n",
        "        return answer\n",
        "    else:\n",
        "        return None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "id": "9a74e19b-a2f0-4d2e-9e67-b138468a308e",
      "metadata": {
        "id": "9a74e19b-a2f0-4d2e-9e67-b138468a308e"
      },
      "outputs": [],
      "source": [
        "def answer_question_factual(question):\n",
        "\n",
        "    try:\n",
        "\n",
        "        # Process question to get parameters\n",
        "        params = process_question_factual(question, nodes, predicates)\n",
        "\n",
        "        print(f\"Parameters found: {params}\\n\")\n",
        "\n",
        "        # Generate the SPARQL query based on the extracted parameters\n",
        "        sparql_query = generate_sparql_query(params)\n",
        "\n",
        "        # Execute the query on the knowledge graph\n",
        "        answer = query_graph(graph, sparql_query)\n",
        "\n",
        "        # Check if an answer was returned\n",
        "        if answer:\n",
        "            return answer\n",
        "        else:\n",
        "            return None\n",
        "\n",
        "    except Exception as e:\n",
        "\n",
        "        return None\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cf1273fa-ea5c-4bee-abd5-623fdb2a538f",
      "metadata": {
        "id": "cf1273fa-ea5c-4bee-abd5-623fdb2a538f"
      },
      "source": [
        "# Final Chatbot Structure\n",
        "\n",
        "We have successfully implemented a way to retrieve embeddings answers and factual answers. Now we put everything together and implement a way for the chatbot to seem as human as possible"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "id": "daaf6ed8-f10f-4786-9386-460bad67c514",
      "metadata": {
        "id": "daaf6ed8-f10f-4786-9386-460bad67c514"
      },
      "outputs": [],
      "source": [
        "def generate_answer(graph, question):\n",
        "\n",
        "    questions_list = split_questions(question)\n",
        "\n",
        "\n",
        "    Answer = \"\"\n",
        "\n",
        "    if len(questions_list) > 1:\n",
        "\n",
        "        for i, q in enumerate(questions_list):\n",
        "\n",
        "            embedding_answer = answer_question_embeddings(q)\n",
        "\n",
        "            print(\"\\n\\n-----------\\n\\n\")\n",
        "\n",
        "            factual_answer = answer_question_factual(q)\n",
        "\n",
        "            print(\"\\n\\n-----------\\n\\n\")\n",
        "\n",
        "            Answer += f\"For question {q} the answer suggested by the embeddings is: {embedding_answer} while the answer obtained by quering the graph is: {factual_answer}\\n\"\n",
        "\n",
        "    else:\n",
        "\n",
        "        if answer_question_embeddings(question):\n",
        "\n",
        "            embedding_answer = answer_question_embeddings(question)\n",
        "\n",
        "            Answer += f\"The answer suggested by the embeddings is: {embedding_answer}\\n\"\n",
        "\n",
        "        print(\"\\n\\n-----------\\n\\n\")\n",
        "\n",
        "        if answer_question_factual(question):\n",
        "\n",
        "            factual_answer = answer_question_factual(question)\n",
        "\n",
        "            Answer += f\"The answer obtained by querying the graph is: {factual_answer}\\n\"\n",
        "\n",
        "        else:\n",
        "            # Generate a response message with pegasus\n",
        "            num_beams = 10\n",
        "            num_return_sequences = 1\n",
        "            context = f\"{question} i don't know\"\n",
        "            print(context)\n",
        "            print(get_response(context,num_return_sequences,num_beams)[0])\n",
        "            print(\"\\n\")\n",
        "            Answer += get_response(context,num_return_sequences,num_beams)[0]\n",
        "\n",
        "\n",
        "        print(\"\\n\\n-----------\\n\\n\")\n",
        "\n",
        "    return Answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "id": "1ab394d7-f925-48a9-bf09-a9aee3f63e97",
      "metadata": {
        "id": "1ab394d7-f925-48a9-bf09-a9aee3f63e97",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0027fdd-664e-4ace-df40-05588bea0e3c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question after preprocessing: Who is the director of Star Wars and who is the screenwriter of The Godfather\n",
            "\n",
            "Extracted entity: Star Wars\n",
            "\n",
            "Extracted entity: The Godfather\n",
            "\n",
            "Question after removing entities: who is the director of and who is the screenwriter of\n",
            "\n",
            "Question after preprocessing: Who is the director of Star Wars and who is the screenwriter of The Godfather\n",
            "\n",
            "Extracted entity: Star Wars\n",
            "\n",
            "Extracted entity: The Godfather\n",
            "\n",
            "Question after removing entities: who is the director of and who is the screenwriter of\n",
            "\n",
            "\n",
            "\n",
            "-----------\n",
            "\n",
            "\n",
            "pattern = who is the (?P<relation>.+?) of (?P<entity>.+)\n",
            "Question matched to pattern who\n",
            "\n",
            "Who is the director of Star Wars and who is the screenwriter of The Godfather i don't know\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `1.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I don't know who is the director of Star Wars and who is the writer of The Godfather.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "-----------\n",
            "\n",
            "\n",
            "The answer suggested by the embeddings is: ['George Lucas', 'Anthony Daniels', 'Andy Secombe']\n",
            "I don't know who is the director of Star Wars and who is the writer of The Godfather.\n"
          ]
        }
      ],
      "source": [
        "# Test\n",
        "\n",
        "question = \"Who is the director of Star Wars and who is the screenwriter of The Godfather\"\n",
        "\n",
        "print(generate_answer(graph, question))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question6 = \"Given that I like The Lion King, Pocahontas, and The Beauty and the Beast, can you recommend some movies?\"\n",
        "\n",
        "#print(generate_answer(graph, question1))\n",
        "print(generate_answer(graph, question6))\n"
      ],
      "metadata": {
        "id": "lwkYGSZv1V2_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f021049-365b-4ab5-e97b-e2a1d0a80636"
      },
      "id": "lwkYGSZv1V2_",
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question after preprocessing: Given that I like The Lion King, Pocahontas, and The Beauty and the Beast, can you recommend some movies?\n",
            "\n",
            "Extracted entity: The Lion King\n",
            "\n",
            "Extracted entity: Pocahontas\n",
            "\n",
            "Extracted entity: The Beauty and the Beast\n",
            "\n",
            "Question after removing entities: given that i like , , and , can you recommend some movies?\n",
            "\n",
            "Liked movies: ['The Lion King', 'Pocahontas', 'Beauty and the Beast']\n",
            "\n",
            "Question after preprocessing: Given that I like The Lion King, Pocahontas, and The Beauty and the Beast, can you recommend some movies?\n",
            "\n",
            "Extracted entity: The Lion King\n",
            "\n",
            "Extracted entity: Pocahontas\n",
            "\n",
            "Extracted entity: The Beauty and the Beast\n",
            "\n",
            "Question after removing entities: given that i like , , and , can you recommend some movies?\n",
            "\n",
            "Liked movies: ['The Lion King', 'Pocahontas', 'Beauty and the Beast']\n",
            "\n",
            "\n",
            "\n",
            "-----------\n",
            "\n",
            "\n",
            "pattern = who is the (?P<relation>.+?) of (?P<entity>.+)\n",
            "pattern = (?:find|which) movies.*contain(?:s)?(?: the word)? (?P<word>\\w+)\n",
            "pattern = (?:find|which) movies with (?P<word>\\w+) in (?:their )?titles?\n",
            "pattern = (?:find|which) movies (?:whose )?(?:title|name) contains? (?P<word>\\w+)\n",
            "pattern = (?:what are|list)(?: the)?(?: highest[-\\s]rated)? movies(?: rated)?(?: above| greater than)?(?: (?P<number>\\d+(\\.\\d+)?))?\n",
            "pattern = movies (?:rated )?(?:above )?(?P<number>\\d+(\\.\\d+)?)?\n",
            "pattern = (?:what are|list)(?: the)?(?: lowest[-\\s]rated)? movies(?: rated)?(?: below| less than)?(?: (?P<number>\\d+(\\.\\d+)?))?\n",
            "pattern = movies (?:rated )?(?:below )?(?P<number>\\d+(\\.\\d+)?)?\n",
            "pattern = which (?P<entity>.+) comes first alphabetically\n",
            "pattern = list (?P<entity>.+) in alphabetical order\n",
            "pattern = which (?P<entity>.+) comes last alphabetically\n",
            "pattern = list (?P<entity>.+) in reverse alphabetical order\n",
            "Parameters found: None\n",
            "\n",
            "Given that I like The Lion King, Pocahontas, and The Beauty and the Beast, can you recommend some movies? i don't know\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `1.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Can you recommend some movies that I like?\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "-----------\n",
            "\n",
            "\n",
            "The answer suggested by the embeddings is: [\"The Lion King II: Simba's Pride\", 'The Lion Guard', 'The King', 'The Little Mermaid', 'The Great Mouse Detective']\n",
            "Can you recommend some movies that I like?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fb16d316-57c4-4c9d-99ec-b8d41738bfa3",
      "metadata": {
        "id": "fb16d316-57c4-4c9d-99ec-b8d41738bfa3"
      },
      "source": [
        "# SpeakEasy Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "id": "a31630b9-a284-4944-a693-ad3e8cc549d3",
      "metadata": {
        "id": "a31630b9-a284-4944-a693-ad3e8cc549d3"
      },
      "outputs": [],
      "source": [
        "from rdflib.namespace import Namespace, RDF, RDFS, XSD\n",
        "from rdflib.term import URIRef, Literal\n",
        "import rdflib"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install '/content/drive/MyDrive/Speakeasy_Project/speakeasy-python-client-library/dist/speakeasypy-1.0.0-py3-none-any.whl'\n",
        "\n",
        "from speakeasypy import Speakeasy, Chatroom\n",
        "from typing import List\n",
        "import time\n",
        "\n",
        "DEFAULT_HOST_URL = 'https://speakeasy.ifi.uzh.ch'\n",
        "listen_freq = 2\n",
        "\n",
        "\n",
        "class Agent:\n",
        "    def __init__(self, username, password):\n",
        "        self.username = username\n",
        "        # Initialize the Speakeasy Python framework and login.\n",
        "        self.speakeasy = Speakeasy(host=DEFAULT_HOST_URL, username=username, password=password)\n",
        "        self.speakeasy.login()  # This framework will help you log out automatically when the program terminates.\n",
        "\n",
        "    def listen(self):\n",
        "        graph = rdflib.Graph()\n",
        "        #graph.parse('/Users/gianmarcoalbano/Desktop/Advanced topics in AI/Speakeasy Project/Datasets/14_graph.nt', format='turtle')\n",
        "        graph.parse('/content/drive/MyDrive/14_graph.nt', format='turtle')\n",
        "        while True:\n",
        "            # only check active chatrooms (i.e., remaining_time > 0) if active=True.\n",
        "            rooms: List[Chatroom] = self.speakeasy.get_rooms(active=True)\n",
        "            for room in rooms:\n",
        "                if not room.initiated:\n",
        "                    # send a welcome message if room is not initiated\n",
        "                    room.post_messages(f'Hello! This is a welcome message from {room.my_alias}.')\n",
        "                    room.initiated = True\n",
        "                # Retrieve messages from this chat room.\n",
        "                # If only_partner=True, it filters out messages sent by the current bot.\n",
        "                # If only_new=True, it filters out messages that have already been marked as processed.\n",
        "                for message in room.get_messages(only_partner=True, only_new=True):\n",
        "                    print(\n",
        "                        f\"\\t- Chatroom {room.room_id} \"\n",
        "                        f\"- new message #{message.ordinal}: '{message.message}' \"\n",
        "                        f\"- {self.get_time()}\")\n",
        "\n",
        "                    # Implement your agent here #\n",
        "                    result = generate_answer(graph, message.message)\n",
        "\n",
        "                    # Send a message to the corresponding chat room using the post_messages method of the room object.\n",
        "                    room.post_messages(f\"Received your message: '{result}' \")\n",
        "                    # Mark the message as processed, so it will be filtered out when retrieving new messages.\n",
        "                    room.mark_as_processed(message)\n",
        "\n",
        "                # Retrieve reactions from this chat room.\n",
        "                # If only_new=True, it filters out reactions that have already been marked as processed.\n",
        "                for reaction in room.get_reactions(only_new=True):\n",
        "                    print(\n",
        "                        f\"\\t- Chatroom {room.room_id} \"\n",
        "                        f\"- new reaction #{reaction.message_ordinal}: '{reaction.type}' \"\n",
        "                        f\"- {self.get_time()}\")\n",
        "\n",
        "                    # Implement your agent here #\n",
        "\n",
        "                    room.post_messages(f\"Received your reaction: '{reaction.type}' \")\n",
        "                    room.mark_as_processed(reaction)\n",
        "\n",
        "            time.sleep(listen_freq)\n",
        "\n",
        "    @staticmethod\n",
        "    def get_time():\n",
        "        return time.strftime(\"%H:%M:%S, %d-%m-%Y\", time.localtime())\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    demo_bot = Agent(\"swift-comet\", \"X2wqU6D3\")\n",
        "    demo_bot.listen()"
      ],
      "metadata": {
        "id": "XzcNaTfqsPcg"
      },
      "id": "XzcNaTfqsPcg",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}