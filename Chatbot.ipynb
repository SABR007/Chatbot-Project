{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d67aee85-f8c9-4281-b9a0-5e9ada39e953",
   "metadata": {},
   "source": [
    "### Import the knowledge graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4443faac-5648-49c6-bdd3-896595ccd507",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdflib.term import URIRef, Literal\n",
    "import rdflib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84ea2151-a884-4115-8288-7d1254e8b8fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Graph identifier=N0af5a034821f41829c12fcba854e7a1f (<class 'rdflib.graph.Graph'>)>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph = rdflib.Graph()\n",
    "graph.parse('/Users/gianmarcoalbano/Desktop/Advanced topics in AI/Speakeasy Project/Datasets/14_graph.nt', format='turtle')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5518924f-0463-4cb7-ae40-54a2de8cbce6",
   "metadata": {},
   "source": [
    "### Some info on the knowledge graph\n",
    "\n",
    "The entities are stored with different URIs. The most common namespaces are the following:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3863b05-3984-4087-8edf-ad588fe73672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define some prefixes\n",
    "WD = rdflib.Namespace('http://www.wikidata.org/entity/')\n",
    "WDT = rdflib.Namespace('http://www.wikidata.org/prop/direct/')\n",
    "DDIS = rdflib.Namespace('http://ddis.ch/atai/')\n",
    "RDFS = rdflib.namespace.RDFS\n",
    "SCHEMA = rdflib.Namespace('http://schema.org/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e5c6022-462a-4729-a5ee-0db67861b59c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some subjects from the knowledge graph\n",
      "http://www.wikidata.org/entity/Q44362\n",
      "http://www.wikidata.org/entity/Q707064\n",
      "http://www.wikidata.org/entity/Q399495\n",
      "http://www.wikidata.org/entity/Q1657593\n",
      "http://www.wikidata.org/entity/Q1209782\n",
      "http://www.wikidata.org/entity/Q1410667\n",
      "http://www.wikidata.org/entity/Q51596819\n",
      "http://www.wikidata.org/entity/Q1347014\n",
      "http://www.wikidata.org/entity/Q42337579\n",
      "http://www.wikidata.org/entity/Q266451\n",
      "\n",
      " Some objects from the knowledge graph\n",
      "https://commons.wikimedia.org/wiki/File:Florian_Teichtmeister_Nestroy-Theaterpreis_2015.jpg\n",
      "Oliver Debuschewitz\n",
      "Kazuchika Kise\n",
      "http://www.wikidata.org/entity/Q17274824\n",
      "https://commons.wikimedia.org/wiki/File:Jordan_Todosey_10.jpg\n",
      "2011 film by Martin Scorsese\n",
      "Marion LÃ©crivain\n",
      "https://commons.wikimedia.org/wiki/File:Revolver_Golden_Gods_Awards.jpg\n",
      "Karl T. Wright\n",
      "actor (1902-1976)\n"
     ]
    }
   ],
   "source": [
    "print('Some subjects from the knowledge graph')\n",
    "for objs in list(set(graph.subjects()))[:10]:\n",
    "    print(objs)\n",
    "    \n",
    "print('\\n Some objects from the knowledge graph')\n",
    "for objs in list(set(graph.objects()))[10:20]:\n",
    "    print(objs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f12dba1-7ff1-43b8-a66b-b131ed4ca1a5",
   "metadata": {},
   "source": [
    "Some ways to access the label of an entity in the graph subjects given it's URI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "029c242a-d30f-4255-922e-3a6d90647c54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node http://www.wikidata.org/entity/Q15922298 has label Hector and the Search for Happiness\n"
     ]
    }
   ],
   "source": [
    "for node in graph.subjects():\n",
    "    if graph.value(subject=node, predicate=RDFS.label): # Check if the triple exists\n",
    "        print(f\"node {node} has label {graph.value(subject=node, predicate=RDFS.label)}\")\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc0c3e0-a3c3-4ed2-ac18-724fadaf089a",
   "metadata": {},
   "source": [
    "We want to check if every subject in the graph has a label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f5f0fdd-f91f-40db-98b5-765e04c8ff7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of subjects with a label: 2051387\n",
      "\n",
      "Number of subjects in the graph: 2056777\n",
      "\n",
      "There are 5390 subject entities without a label\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "j = 0\n",
    "for node in graph.subjects():\n",
    "    j += 1\n",
    "    if graph.value(subject=node, predicate=RDFS.label): # Check if the triple exists\n",
    "        i += 1\n",
    "\n",
    "print(f\"Number of subjects with a label: {i}\\n\")\n",
    "print(f\"Number of subjects in the graph: {j}\\n\")\n",
    "if i != j:\n",
    "    print(f\"There are {j-i} subject entities without a label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed26093-bd80-46a4-a55e-4cb5caa5abbe",
   "metadata": {},
   "source": [
    "### Make a dictionary of nodes URIs with the respective labels\n",
    "\n",
    "We want to make a dictionary in which the keys are the nodes URIs and the values are the nodes labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0aa69a55-b3df-4127-a32a-aaf4396d1ae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URI: http://www.wikidata.org/entity/Q44362, Label: Theo Rossi\n"
     ]
    }
   ],
   "source": [
    "# Function to extract the local part of a URI (e.g., after the last / or #)\n",
    "def extract_label_from_uri(uri, namespaces):\n",
    "    # Loop through all namespaces and remove the matching part\n",
    "    for namespace in namespaces:\n",
    "        if str(uri).startswith(str(namespace)):\n",
    "            return str(uri).replace(str(namespace), \"\")\n",
    "    # If no match, return the original URI\n",
    "    return str(uri).split('/')[-1]\n",
    "\n",
    "# Function to build a dictionary of nodes and their labels\n",
    "def build_node_label_dict(graph, namespaces):\n",
    "    nodes = {}\n",
    "    \n",
    "    for node in graph.all_nodes():\n",
    "        if isinstance(node, rdflib.term.URIRef):  # Only process URIs\n",
    "            # Check if the node has a label\n",
    "            label = graph.value(node, RDFS.label)\n",
    "            \n",
    "            if label:\n",
    "                # If label exists, use it\n",
    "                nodes[node.toPython()] = str(label)\n",
    "            else:\n",
    "                # If no label, extract the local part of the URI\n",
    "                local_label = extract_label_from_uri(node, namespaces)\n",
    "                nodes[node.toPython()] = local_label\n",
    "    \n",
    "    return nodes\n",
    "\n",
    "namespaces = [WD, WDT, DDIS, RDFS, SCHEMA]\n",
    "\n",
    "# TODO: change the name of nodes into 'ent2lbl'\n",
    "nodes = build_node_label_dict(graph, namespaces)\n",
    "\n",
    "# Check the result\n",
    "for uri, label in nodes.items():\n",
    "    print(f\"URI: {uri}, Label: {label}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac4d4e9-79cf-480d-892c-193616839313",
   "metadata": {},
   "source": [
    "Make an inverse dictionary to find URIs of the entities given the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "746d40d3-ea9e-4f47-97f3-08d1fc0ffa96",
   "metadata": {},
   "outputs": [],
   "source": [
    "ent2uri = {ent: uri for uri, ent in nodes.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e8a1bb-a5dd-4258-b1d5-cb9573f1fd87",
   "metadata": {},
   "source": [
    "We also make another dictionary specifically for predicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0132845-d9aa-4f67-910a-058e76b108d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URI: http://www.wikidata.org/prop/direct/P161, Label: cast member\n"
     ]
    }
   ],
   "source": [
    "# Function to build a dictionary of predicates and their labels\n",
    "def build_pred_label_dict(graph, namespaces):\n",
    "    predicates = {}\n",
    "    \n",
    "    for node in graph.predicates():\n",
    "        if isinstance(node, rdflib.term.URIRef):  # Only process URIs\n",
    "            # Check if the node has a label\n",
    "            label = graph.value(node, RDFS.label)\n",
    "            \n",
    "            if label:\n",
    "                # If label exists, use it\n",
    "                predicates[node.toPython()] = str(label)\n",
    "\n",
    "            # This condition is never evaluated cause all the predicates have labels\n",
    "            else:\n",
    "                # If no label, extract the local part of the URI\n",
    "                local_label = extract_label_from_uri(node, namespaces)\n",
    "                predicates[node.toPython()] = local_label\n",
    "    \n",
    "    return predicates\n",
    "\n",
    "# TODO: change the name of predicates into 'pred2lbl'\n",
    "predicates = build_pred_label_dict(graph, namespaces)\n",
    "\n",
    "# Check the result\n",
    "for uri, label in predicates.items():\n",
    "    print(f\"URI: {uri}, Label: {label}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16659397-8837-438d-910a-1190c6efbd8e",
   "metadata": {},
   "source": [
    "Make an inverse dictionary to find URIs of the predicates given the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fad9cfe9-044f-4ff1-8213-e9fd11794d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred2uri = {pred: uri for uri, pred in predicates.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5983cba-d0b7-4118-ba53-7025522946ca",
   "metadata": {},
   "source": [
    "### Matching function\n",
    "\n",
    "Suppose we find an entity \"Batman_1989\" in the question we want to answer. However \"Batman_1989\" is registered in the knowledge graph as \"Batman 1989\". We need a function that takes the entity from the questions and finds the closest entity in the knowledge graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ee43819e-5bf4-448b-bdb7-b04515fd089d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import editdistance\n",
    "\n",
    "def match_entity(entity, dictionary=nodes):\n",
    "    \n",
    "    tmp = 9999\n",
    "    match_node = \"\"\n",
    "    match_value = \"\"\n",
    "    \n",
    "    for key, value in dictionary.items():\n",
    "        '''\n",
    "        if editdistance.eval(value, entity) == 0:\n",
    "\n",
    "            return key, value\n",
    "        '''\n",
    "        if editdistance.eval(value, entity) < tmp:\n",
    "            tmp = editdistance.eval(value, entity)\n",
    "            match_node = key\n",
    "            match_value = value\n",
    "    \n",
    "    return match_node, match_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2b0f1b-d9a2-4287-8a3f-f02e9ab2cac6",
   "metadata": {},
   "source": [
    "We can also use the match_entity function to match a predicate to the closest predicate in the graph by specifing dictionary=predicates\n",
    "\n",
    "\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "729b5437-ba55-40f2-afe8-6481e9320c5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URI: http://www.wikidata.org/prop/direct/P57, label: director\n",
      "URI: http://www.wikidata.org/entity/Q15112439, label: Rosie\n"
     ]
    }
   ],
   "source": [
    "match_node, match_value = match_entity('direcror', predicates)\n",
    "print(f\"URI: {match_node}, label: {match_value}\")\n",
    "\n",
    "match_node, match_value = match_entity('movie')\n",
    "print(f\"URI: {match_node}, label: {match_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7aef7e7-9d51-4414-84de-a620ea6b57f8",
   "metadata": {},
   "source": [
    "### Processing questions\n",
    "\n",
    "Our first approach to answer factual question is very naive. The following function takes a question and tries to fit it to a series of questions patterns to extract a relation and an entity. For example the question \"Who is the director of Batman\" corresponds to pattern \"who is the (?P<relation>.*) of (?P<entity>.*)\". When we call method re.match on the question and the pattern it produces a match object (we call it match in the function) that contains a dictionary: {'relation': 'director', 'entity': Batman}. To access this dictionary we call .groupdict() on the match object (so match.groupdict() will be the dictionary). We retrieve relation and entity from the dictionary unsing get('relation') and get('entity') and specifing that if the dictionary doesn't have that key it should output \"\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c3be6ffd-07d6-41f1-897c-72ad56f8361b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\nTODO\\ninclude queries from the olat page of the 1 intermediate evaluation\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "question_patterns = [\n",
    "    \n",
    "    # Pattern 0: who and what\n",
    "    (r\"who is the (?P<relation>.+?) of (?P<entity>.+)\", 'who', 1),\n",
    "\n",
    "    # Pattern 1: Find movies with (word) in their titles\n",
    "    (r\"(?:find|which) movies.*contain(?:s)?(?: the word)? (?P<word>\\w+)\", 'find_word_in_title', 0),\n",
    "    (r\"(?:find|which) movies with (?P<word>\\w+) in (?:their )?titles?\", 'find_word_in_title', 0),\n",
    "    (r\"(?:find|which) movies (?:whose )?(?:title|name) contains? (?P<word>\\w+)\", 'find_word_in_title', 0),\n",
    "    \n",
    "    # Pattern 2: Highest-rated movies above a certain rating\n",
    "    (r\"(?:what are|list)(?: the)?(?: highest-rated)? movies.*(?:rated )?(?:above|greater than) (?P<number>\\d+(\\.\\d+)?)\", 'movies_rating_above', 0),\n",
    "    (r\"movies rated above (?P<number>\\d+(\\.\\d+)?)\", 'movies_rating_above', 0),\n",
    "    \n",
    "    # Pattern 3: Lowest-rated movies below a certain rating\n",
    "    (r\"(?:what are|list)(?: the)?(?: lowest-rated)? movies.*(?:rated )?(?:below|less than) (?P<number>\\d+(\\.\\d+)?)\", 'movies_rating_below', 0),\n",
    "    (r\"movies rated below (?P<number>\\d+(\\.\\d+)?)\", 'movies_rating_below', 0),\n",
    "    \n",
    "    # Pattern 4: Entities in alphabetical order\n",
    "    (r\"which (?P<entity>.+) comes first alphabetically\", 'entity_first_alphabetically', 1),\n",
    "    (r\"list (?P<entity>.+) in alphabetical order\", 'entity_first_alphabetically', 1),\n",
    "    \n",
    "    # Pattern 5: Entities in reverse alphabetical order\n",
    "    (r\"which (?P<entity>.+) comes last alphabetically\", 'entity_last_alphabetically', 1),\n",
    "    (r\"list (?P<entity>.+) in reverse alphabetical order\", 'entity_last_alphabetically', 1),\n",
    "]\n",
    "\n",
    "\"\"\" \n",
    "TODO\n",
    "include queries from the olat page of the 1 intermediate evaluation\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "88aafb41-512c-4780-8c56-6bd27b7c6336",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_question(question, entity_dictionary, predicate_dictionary):\n",
    "    \n",
    "    for pattern, qtype, matching in question_patterns:\n",
    "        match = re.match(pattern, question, re.IGNORECASE)\n",
    "        \n",
    "        if match:\n",
    "            params = match.groupdict()\n",
    "            params['type'] = qtype  # Add the question type to the params\n",
    "            \n",
    "            if matching:\n",
    "                # Extract and match the relation and entity\n",
    "                relation = params.get('relation', \"\").lower()  # Set default as empty string\n",
    "                entity = params.get('entity', \"\") # Set default as empty string (don't lower it)\n",
    "                \n",
    "                # Match the entity to the closest in the knowledge graph\n",
    "                matched_entity_uri, matched_entity_label = match_entity(entity, dictionary=entity_dictionary) if entity else (None, None)\n",
    "                \n",
    "                # Match the relation to the closest in the knowledge graph\n",
    "                matched_predicate_uri, matched_predicate_label = match_entity(relation, dictionary=predicate_dictionary) if relation else (None, None)\n",
    "                \n",
    "                # Add the matched URIs and labels to params\n",
    "                params['matched_entity_uri'] = matched_entity_uri\n",
    "                params['matched_entity_label'] = matched_entity_label\n",
    "                params['matched_predicate_uri'] = matched_predicate_uri\n",
    "                params['matched_predicate_label'] = matched_predicate_label\n",
    "                \n",
    "            return params\n",
    "            \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3cfa56c0-dad0-4fb3-85b3-c61bd3761603",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pattern 0: Question: Who is the director of batman\n",
      "\n",
      "relation : director\n",
      "\n",
      "entity : batman\n",
      "\n",
      "type : who\n",
      "\n",
      "matched_entity_uri : http://www.wikidata.org/entity/Q596699\n",
      "\n",
      "matched_entity_label : Batman\n",
      "\n",
      "matched_predicate_uri : http://www.wikidata.org/prop/direct/P57\n",
      "\n",
      "matched_predicate_label : director\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Pattern 1: Question: Which movies whose name contains italy\n",
      "\n",
      "word : italy\n",
      "\n",
      "type : find_word_in_title\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Pattern 2 not matched\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Pattern 3 not matched\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Pattern 4: Question: Which films comes first alphabetically\n",
      "\n",
      "entity : films\n",
      "\n",
      "type : entity_first_alphabetically\n",
      "\n",
      "matched_entity_uri : http://www.wikidata.org/entity/Q11424\n",
      "\n",
      "matched_entity_label : film\n",
      "\n",
      "matched_predicate_uri : None\n",
      "\n",
      "matched_predicate_label : None\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Pattern 5: Question: list actors in reverse alphabetical order\n",
      "\n",
      "entity : actors\n",
      "\n",
      "type : entity_last_alphabetically\n",
      "\n",
      "matched_entity_uri : http://www.wikidata.org/entity/Q33999\n",
      "\n",
      "matched_entity_label : actor\n",
      "\n",
      "matched_predicate_uri : None\n",
      "\n",
      "matched_predicate_label : None\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "user_input = {\n",
    "    0: \"Who is the director of batman\",\n",
    "    1: \"Which movies whose name contains italy\",\n",
    "    2: \"List the highest rated movies\",\n",
    "    3: \"what are the lowest-rated movies?\",\n",
    "    4: \"Which films comes first alphabetically\",\n",
    "    5: \"list actors in reverse alphabetical order\"\n",
    "}\n",
    "\n",
    "for pattern, question in user_input.items():\n",
    "    \n",
    "    params = process_question(question, nodes, predicates)\n",
    "    \n",
    "    if params:\n",
    "        print(f\"Pattern {pattern}: Question: {question}\\n\")\n",
    "        \n",
    "        for key, value in params.items():\n",
    "            print(f\"{key} : {params[key]}\\n\")\n",
    "        print(\"\\n\\n\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"\\nPattern {pattern} not matched\\n\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "460c0a7d-5cc0-4bdd-805e-6f9a3276d7fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nTODO\\nyou shoud include prefixes in each query!\\n'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_sparql_query(params):\n",
    "    qtype = params.get('type')\n",
    "\n",
    "    if qtype == 'who':\n",
    "        sparql_query = f\"\"\"\n",
    "        SELECT ?result WHERE {{\n",
    "            ?entity rdfs:label \"{params['matched_entity_label']}\"@en .  \n",
    "            ?entity <{params['matched_predicate_uri']}> ?item . \n",
    "            ?item rdfs:label ?result .\n",
    "            FILTER (lang(?result) = 'en')\n",
    "        }}  \n",
    "        \"\"\"\n",
    "        return sparql_query\n",
    "\n",
    "    \n",
    "    # Fix: this query returns names of all the entities whose label contains the word, not just movies\n",
    "    elif qtype == 'find_word_in_title':\n",
    "        word = params.get('word')\n",
    "        sparql_query = f\"\"\"\n",
    "        SELECT ?movieLabel WHERE {{\n",
    "            ?movie rdfs:label ?movieLabel .\n",
    "            FILTER(CONTAINS(LCASE(?movieLabel), LCASE(\"{word}\"))) .\n",
    "            FILTER (lang(?movieLabel) = 'en')\n",
    "        }}\n",
    "        \"\"\"\n",
    "        return sparql_query\n",
    "\n",
    "    elif qtype == 'movies_rating_above':\n",
    "        number = params.get('number')\n",
    "        sparql_query = f\"\"\"\n",
    "        SELECT ?movieLabel WHERE {{\n",
    "            ?movie ddis:rating ?rating .\n",
    "            FILTER(?rating > {number}) .\n",
    "            ?movie rdfs:label ?movieLabel .\n",
    "            FILTER (lang(?movieLabel) = 'en')\n",
    "        }} ORDER BY DESC(?rating) LIMIT 1\n",
    "        \"\"\"\n",
    "        return sparql_query\n",
    "\n",
    "    elif qtype == 'movies_rating_below':\n",
    "        number = params.get('number')\n",
    "        sparql_query = f\"\"\"\n",
    "        SELECT ?movieLabel WHERE {{\n",
    "            ?movie ddis:rating ?rating .\n",
    "            FILTER(?rating < {number}) .\n",
    "            ?movie rdfs:label ?movieLabel .\n",
    "            FILTER (lang(?movieLabel) = 'en')\n",
    "        }} ORDER BY DESC(?rating)\n",
    "        \"\"\"\n",
    "        return sparql_query\n",
    "\n",
    "    elif qtype == 'entity_first_alphabetically':\n",
    "        sparql_query = f\"\"\"\n",
    "        SELECT ?entity_label WHERE {{\n",
    "            ?entity wdt:P31 <{params['matched_entity_uri']}> .\n",
    "            ?entity rdfs:label ?entity_label .\n",
    "            FILTER (lang(?entity_label) = 'en')\n",
    "        }} ORDER BY ASC(?entity_label)\n",
    "        \"\"\"\n",
    "        return sparql_query\n",
    "\n",
    "    elif qtype == 'entity_last_alphabetically':\n",
    "        sparql_query = f\"\"\"\n",
    "        SELECT ?entity_label WHERE {{\n",
    "            ?entity wdt:P31 <{params['matched_entity_uri']}> .\n",
    "            ?entity rdfs:label ?entity_label .\n",
    "            FILTER (lang(?entity_label) = 'en')\n",
    "        }} ORDER BY DESC(?entity_label)\n",
    "        \"\"\"\n",
    "        return sparql_query\n",
    "\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "'''\n",
    "TODO\n",
    "you shoud include prefixes in each query!\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ffde013-803b-41a7-a96a-3bb79fa798e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "user_input = {\n",
    "    0: \"Who is the director of batman\",\n",
    "    1: \"Which movies whose name contains italy\",\n",
    "    2: \"List the highest rated movies\",\n",
    "    3: \"what are the lowest-rated movies?\",\n",
    "    4: \"Which films comes first alphabetically\",\n",
    "    5: \"list actors in reverse alphabetical order\"\n",
    "}\n",
    "\n",
    "for pattern, question in user_input.items():\n",
    "    \n",
    "    params = process_question(question, nodes, predicates)\n",
    "\n",
    "    if params:\n",
    "\n",
    "        sparql_query = generate_sparql_query(params)\n",
    "\n",
    "        print(f\"Question: {question} has generated query:\\n{sparql_query}\\n\")\n",
    "\n",
    "        print(f\"Checking if the query runs on the graph:\\n\")\n",
    "\n",
    "        # Check if SPARQL query runs on the graph\n",
    "        res = graph.query(sparql_query)\n",
    "        \n",
    "        if res:\n",
    "            '''\n",
    "            print(res)\n",
    "            results = []\n",
    "            for row in res:\n",
    "                results.append(row.result)\n",
    "            for result in results:\n",
    "                print(f\"Answer: {result}\")\n",
    "            print(\"\\n\")\n",
    "            '''\n",
    "            result = [str(s) for s, in graph.query(sparql_query)]\n",
    "            print(result)\n",
    "            \n",
    "        else:\n",
    "            print(\"QUERY NOT WORKING\\n\")\n",
    "\n",
    "    else:\n",
    "\n",
    "        print(f\"pattern {pattern} has failed the parameter matching\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "194354d5-cece-41fa-8ad8-7a0322201365",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        SELECT ?result WHERE {\n",
      "            ?entity rdfs:label \"Apocalypse Now\"@en .  \n",
      "            ?entity <http://www.wikidata.org/prop/direct/P57> ?item . \n",
      "            ?item rdfs:label ?result .\n",
      "            FILTER (lang(?result) = 'en')\n",
      "        }  \n",
      "        \n",
      "Answer: Francis Ford Coppola\n"
     ]
    }
   ],
   "source": [
    "# Example Usage\n",
    "user_input = \"who is the director of Apocalypse Now\"\n",
    "params = process_question(user_input, nodes, predicates)\n",
    "sparql_query = generate_sparql_query(params)\n",
    "print(sparql_query)\n",
    "\n",
    "# Check if SPARQL query runs on the graph\n",
    "res = graph.query(sparql_query)\n",
    "results = []\n",
    "for row in res:\n",
    "    results.append(row.result)\n",
    "if results:\n",
    "    for result in results:\n",
    "        print(f\"Answer: {result}\")\n",
    "else:\n",
    "    print(\"AAAA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3839ed9e-1953-4e34-bfe1-d13bb62db4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_graph(graph, sparql_query):\n",
    "    \n",
    "    print(\"--- SPARQL query ---\")\n",
    "    print(sparql_query)\n",
    "\n",
    "    # Execute the query\n",
    "    qres = graph.query(sparql_query)\n",
    "\n",
    "    # Process the results\n",
    "    results = []\n",
    "    for row in qres:\n",
    "        results.append(row.result)\n",
    "    \n",
    "    # Check if we have results, if not return a friendly message\n",
    "    if results:\n",
    "        for result in results:\n",
    "            print(f\"Answer: {result}\")\n",
    "    else:\n",
    "        print(\"No results found for the given query.\")\n",
    "        return []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e0051817-8d19-43f1-9b80-d2cf62a6714b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(graph, question):\n",
    "\n",
    "    params = process_question(question, nodes, predicates)\n",
    "\n",
    "    sparql_query = generate_sparql_query(params)\n",
    "\n",
    "    answer = query_graph(graph, sparql_query)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e12baa0-a05f-4a83-9b72-b8cb334a6b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = \"who is the director of Apocalypse Now\"\n",
    "generate_answer(graph, user_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5de3ea3-906e-43fa-90f7-8dff056de08a",
   "metadata": {},
   "source": [
    "One problem with this is that relation \"release date\" is not correctly catched because it is closer to relation \"relative\" than it is to relation \"publication date\" which actually exists as a predicate in the knowledge graph\n",
    "\n",
    "IDEA TO FIX IT: modify the matching function for the predicates so that instead of relying on the edit distance it relies on embeddings similarity. Come back to this when you have started implementing embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11364b72-559d-4a6e-8c43-7df59a076234",
   "metadata": {},
   "source": [
    "## Embeddings\n",
    "\n",
    "We will now implement an approach that relies on embeddings rather than querying the graph directly. For this we will need to extract entities from the graph in a more dynamic way and will resort to NER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2347fc4-9525-4bcc-9656-ea8f8024c0b4",
   "metadata": {},
   "source": [
    "### NER\n",
    "\n",
    "We choose model 'Babelscape' because it was already trained on a large wikidata dataset and it is by far the best at recognizing movie titles as 'MISC'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "0f8c7a98-afc6-4053-a4bc-433f52f974ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Babelscape/wikineural-multilingual-ner\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"Babelscape/wikineural-multilingual-ner\")\n",
    "\n",
    "ner_pipeline = pipeline(\"ner\", model=model, tokenizer=tokenizer, grouped_entities=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a66f7d-193e-4d80-8070-d86a42143c1e",
   "metadata": {},
   "source": [
    "### Synonyms handling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0c89f5-204e-44b5-a395-3cf556e627d5",
   "metadata": {},
   "source": [
    "To account for the presence of synonyms in the question we decided to implement a model that computes the similarity between a phrase and the list of predicates from the knowledge graph and returns the most similar matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "87629c72-b4f2-432a-a9dd-e36e0bd3d31e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-md==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.8.0/en_core_web_md-3.8.0-py3-none-any.whl (33.5 MB)\n",
      "\u001b[2K     \u001b[90mââââââââââââââââââââââââââââââââââââââââ\u001b[0m \u001b[32m33.5/33.5 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h\u001b[38;5;2mâ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_md')\n",
      "\u001b[38;5;3mâ  Restart to reload dependencies\u001b[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# this command downloads the Spacy model\n",
    "spacy.cli.download(\"en_core_web_md\")\n",
    "    \n",
    "nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "07ce4318-a85e-42ec-b675-696284d04973",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_match(phrase, predicate_dict, n=5, confidence=0.6):\n",
    "    \"\"\"\n",
    "    Given a phrase, a dictionary of predicate values, an integer n, and a confidence threshold,\n",
    "    return the top n most similar words to the phrase from the dictionary values\n",
    "    that have a similarity score above the confidence threshold.\n",
    "    \"\"\"\n",
    "    phrase_token = nlp(phrase)\n",
    "    similarities = []\n",
    "\n",
    "    # Calculate similarity between phrase and each predicate value\n",
    "    for predicate in predicate_dict.values():\n",
    "        predicate_token = nlp(predicate)\n",
    "        similarity = phrase_token.similarity(predicate_token)\n",
    "        \n",
    "        # Only consider matches above the confidence threshold\n",
    "        if similarity > confidence:\n",
    "            similarities.append((predicate, similarity))\n",
    "\n",
    "    # Sort by similarity in descending order and get the top n matches\n",
    "    top_n_matches = sorted(similarities, key=lambda x: x[1], reverse=True)[:n]\n",
    "    \n",
    "    # Return only the most similar words\n",
    "    return [match[0] for match in top_n_matches]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faec479c-e71c-41c3-b6ca-e430b7c0a482",
   "metadata": {},
   "source": [
    "Some weakness of this methods: \"Children\" is not correctly associated to predicate \"Child\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "4633d4e7-45cf-4065-ad8d-584dec6031e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['collection', 'publication date', 'student of']\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "phrase = \"release date\"\n",
    "n = 3\n",
    "print(find_match(phrase, predicates, n))  # Should return the top 3 most similar predicates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defd30df-100e-4437-893d-7d4875e308c1",
   "metadata": {},
   "source": [
    "### Extracting Predicates\n",
    "\n",
    "We have implemented the following pipeline to extract predicates from the question:\n",
    "\n",
    "- Extract meaningful words from the question with spacy\n",
    "    - For exaple: from question 'who directed...' only 'directed' is extracted\n",
    "- Generate ngrams from meaningful words\n",
    "    - If the predicate is made of 2 words like \"publication date\" the meaningful word would be ['publication', 'date'] which would not be mapped to 'publication date' but to other words. This is why we generate a list of ngrams like [\"publication 'date\", \"publication\", \"date\"].\n",
    "- Starting with the longest ngram, try to find the predicate from the predicate list that is closest to the ngram. If a match is found, we return it. This means that we prioritize matching longest ngrams\n",
    "    - Before we compare the ngram to the list of predicates we lemmatize it and turn it into a noun using the verb_to_noun dictionary we wrote. This is because many predicates in the list are in the form \"director\", \"writer\" instead of \"direct\" and \"write\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "38618fcc-6490-42fd-95cb-a0518d4694e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "verb_to_noun = {\n",
    "    \"affiliate\": \"affiliation\",\n",
    "    \"animate\": \"animator\",\n",
    "    \"base\": \"based on\",\n",
    "    \"cast\": \"cast member\",\n",
    "    \"characterize\": \"characters\",\n",
    "    \"depict\": \"depicts\",\n",
    "    \"describe\": \"node description\",\n",
    "    \"design\": \"designed by\",\n",
    "    \"distribute\": \"distributed by\",\n",
    "    \"educate\": \"educated at\",\n",
    "    \"employ\": \"employer\",\n",
    "    \"found\": \"founded by\",\n",
    "    \"influence\": \"influenced by\",\n",
    "    \"locate\": \"location\",\n",
    "    \"narrate\": \"narrator\",\n",
    "    \"originate\": \"country of origin\",\n",
    "    \"participate\": \"participant in\",\n",
    "    \"perform\": \"performer\",\n",
    "    \"produce\": \"producer\",\n",
    "    \"publish\": \"publication date\",\n",
    "    \"rate\": \"rating\",\n",
    "    \"receive\": \"award received\",\n",
    "    \"represent\": \"represented by\",\n",
    "    \"screen\": \"screenwriter\",\n",
    "    \"study\": \"student of\",\n",
    "    \"write\": \"screenwriter\",\n",
    "    \"direct\": \"director\",\n",
    "    \"photograph\": \"director of photography\",\n",
    "    \"edit\": \"film editor\",\n",
    "    \"speak\": \"languages spoken, written or signed\",\n",
    "    \"produce\": \"production company\",\n",
    "    \"confer\": \"conferred by\",\n",
    "    \"broadcast\": \"broadcast by\",\n",
    "    \"present\": \"presented in\",\n",
    "    \"voice\": \"voice actor\",\n",
    "    \"film\": \"filming location\",\n",
    "    \"release\": \"publication date\",\n",
    "    \"award\": \"award received\",\n",
    "    \"create\": \"creator\",\n",
    "    \"develop\": \"developer\",\n",
    "    \"choreograph\": \"choreographer\",\n",
    "    \"make\": \"production company\",\n",
    "    \"assemble\": \"crew member(s)\",\n",
    "    \"inspire\": \"inspired by\",\n",
    "    \"contribute\": \"contributor to the creative work or subject\",\n",
    "    \"style\": \"costume designer\",\n",
    "    \"nominate\": \"nominated for\",\n",
    "    \"portray\": \"cast member\",\n",
    "    \"describe\": \"node description\",\n",
    "    \"label\": \"node label\",\n",
    "    \"set\": \"narrative location\",\n",
    "    \"shot\": \"filming location\",\n",
    "    \"main character\" : \"characters\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "94979c4d-d76d-405d-b105-d99637ae9f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if some values in the dict do not correspond to actual entities in the graph\n",
    "for value in verb_to_noun.values():\n",
    "    if value not in predicates.values():\n",
    "        print(f\"{value} to be deleted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "b8736687-1fb3-4e6a-950d-e64b5d84550a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_relation(sentence, predicate_dict, n=5, confidence=0.6, max_ngram_size=3):\n",
    "    \"\"\"\n",
    "    Extracts the relation from a sentence by finding the most similar predicates,\n",
    "    prioritizing longer n-grams first. If a match with similarity > confidence\n",
    "    is found, it returns that result immediately.\n",
    "    \n",
    "    Args:\n",
    "    - sentence (str): The input sentence from which to extract the relation.\n",
    "    - predicate_dict (dict): Dictionary of known predicates with their descriptions.\n",
    "    - n (int): Number of top matches to return.\n",
    "    - confidence (float): Minimum similarity threshold for a match.\n",
    "    - max_ngram_size (int): Maximum number of words in an n-gram to consider for matching.\n",
    "    \n",
    "    Returns:\n",
    "    - list: Top `n` predicate matches that have a similarity score above the confidence threshold.\n",
    "    \"\"\"\n",
    "    # Step 1: Parse the sentence to filter stop words and prioritize key phrases\n",
    "    doc = nlp(sentence)\n",
    "    meaningful_words = [token.text for token in doc if not token.is_stop and token.is_alpha]\n",
    "    \n",
    "    # Step 2: Generate prioritized n-grams from meaningful words (starting with the longest n-grams)\n",
    "    ngrams = []\n",
    "    for size in range(max_ngram_size, 0, -1):  # Start with larger n-grams\n",
    "        ngrams += [\" \".join(meaningful_words[i:i+size]) for i in range(len(meaningful_words) - size + 1)]\n",
    "    \n",
    "    # Step 3: Check each n-gram for similarity, starting with the longest\n",
    "    for ngram in ngrams:\n",
    "\n",
    "        # First check if the ngram corresponds exactly or almost exactly to a predicate using the editdistance:\n",
    "        if match_entity_editdistance(ngram, dictionary=predicates, threshold=2):\n",
    "            match_node, match_value, _ = match_entity_editdistance(ngram, dictionary=predicates, threshold=2)\n",
    "            return [match_value]\n",
    "        \n",
    "        print(f\"ngram: {ngram}\")\n",
    "        ngram = \" \".join([verb_to_noun.get(token.lemma_, token.lemma_) for token in nlp(ngram)])\n",
    "        print(f\"lemma: {ngram}\\n\")\n",
    "        \n",
    "        matches = find_match(ngram, predicate_dict, n=n, confidence=confidence)\n",
    "        \n",
    "        # If a match with similarity > confidence is found, return immediately\n",
    "        if matches:\n",
    "            return matches\n",
    "    \n",
    "    # Step 4: If no matches above the confidence threshold are found, return an empty list\n",
    "    return []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "c32895f7-04da-4f66-984c-d6090b0e0174",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ngram: release date\n",
      "lemma: publication date date\n",
      "\n",
      "Extracted Relation: ['publication date', 'place of publication', 'student of']\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Who is the release date of ()?\"\n",
    "relation = extract_relation(sentence, predicates, n=3, confidence=0.5)\n",
    "print(\"Extracted Relation:\", relation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d84f693-4c08-4653-9c2c-d8258081b20b",
   "metadata": {},
   "source": [
    "### Entities handling\n",
    "\n",
    "For entities the problem of syninyms is not that relevent because generally we can assume that people's names and movie's titles have no synonims. However we still need to make sure that the entities recognized by the NER algorithm correspond to real entities in the knowledge graph, otherwise we cannot map them to an embedding. To achieve this we can use the match_entity function based on editdistance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "93ca671f-8574-4add-a083-9476efd88e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import editdistance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "7ec40fca-512a-4e05-9cc6-69b8d42ab70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_entity_editdistance(entity, dictionary=nodes, threshold=5):\n",
    "    \"\"\"\n",
    "    Matches the given entity to the closest node in the dictionary based on edit distance.\n",
    "    Returns None if the closest match exceeds the specified distance threshold.\n",
    "    \n",
    "    Args:\n",
    "    - entity (str): The entity to match.\n",
    "    - dictionary (dict): The graph dictionary with nodes to match against.\n",
    "    - threshold (int): The maximum allowable edit distance for a match.\n",
    "    \n",
    "    Returns:\n",
    "    - (str, str) or (None, None): Returns (node_key, node_value) if a match is found within the threshold,\n",
    "      otherwise returns (None, None).\n",
    "    \"\"\"\n",
    "    tmp = float('inf')  # Start with the highest possible distance\n",
    "    match_node = None\n",
    "    match_value = None\n",
    "    \n",
    "    for key, value in dictionary.items():\n",
    "        # Calculate edit distance between the entity and current node value\n",
    "        distance = editdistance.eval(value, entity)\n",
    "        if distance < tmp:\n",
    "            tmp = distance\n",
    "            match_node = key\n",
    "            match_value = value\n",
    "    \n",
    "    # Return None if the closest match exceeds the threshold\n",
    "    if tmp > threshold:\n",
    "        return None\n",
    "    \n",
    "    return match_node, match_value, tmp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "665cf3c2-1bf4-4ba7-ae24-07d3a153079b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('http://www.wikidata.org/entity/Q25188', 'Inception', 1)\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "phrase = \"Incption\"\n",
    "print(match_entity_editdistance(phrase, nodes)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18871b8-cd18-4dc8-ab9e-5a873c0d356e",
   "metadata": {},
   "source": [
    "### Extract embeddings from the files\n",
    "\n",
    "We extract embeddings from the files. We will explain how to use them after the process_question function. Since there is a problem with the relation embeddings we need to extract them now and account for that in the process_question function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "d908c618-8b7a-4ac0-ae5e-24d758a51e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "entity_matrix = np.load('/Users/gianmarcoalbano/Desktop/Advanced topics in AI/Chatbot-Project/ddis-graph-embeddings/entity_embeds.npy')\n",
    "predicate_matrix = np.load('/Users/gianmarcoalbano/Desktop/Advanced topics in AI/Chatbot-Project/ddis-graph-embeddings/relation_embeds.npy')\n",
    "\n",
    "with open('/Users/gianmarcoalbano/Desktop/Advanced topics in AI/Chatbot-Project/ddis-graph-embeddings/entity_ids.del') as ifile:\n",
    "    ent2id = {ent: int(idx) for idx, ent in csv.reader(ifile, delimiter='\\t')}\n",
    "    id2ent = {v: k for k, v in ent2id.items()}\n",
    "with open('/Users/gianmarcoalbano/Desktop/Advanced topics in AI/Chatbot-Project/ddis-graph-embeddings/relation_ids.del') as ifile:\n",
    "    pred2id = {rel: int(idx) for idx, rel in csv.reader(ifile, delimiter='\\t')}\n",
    "    id2pred = {v: k for k, v in pred2id.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ceafa6-7190-4372-bc0a-b30315b2be91",
   "metadata": {},
   "source": [
    "### Predicates without embeddings\n",
    "\n",
    "There seems to be a problem with the embeddings. Some of them are missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "462cbdad-4149-4094-b0d6-1ae87cdbfeaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicates list: 255\n",
      "relation embeddings list: 248\n",
      "\n",
      "node label has no embedding\n",
      "IMDb ID has no embedding\n",
      "image has no embedding\n",
      "tag has no embedding\n",
      "node description has no embedding\n",
      "publication date has no embedding\n",
      "box office has no embedding\n",
      "rating has no embedding\n"
     ]
    }
   ],
   "source": [
    "print(f\"predicates list: {len(predicates)}\")\n",
    "print(f\"relation embeddings list: {len(predicate_matrix)}\\n\")\n",
    "#pred2id['http://www.wikidata.org/prop/direct/P577']\n",
    "pred_without_embeddings = []\n",
    "# Which predicates are missing an embedding?\n",
    "for predicate in predicates.values():\n",
    "    try:\n",
    "        id = pred2id[pred2uri[predicate]]\n",
    "    except KeyError:\n",
    "        print(f\"{predicate} has no embedding\")\n",
    "        pred_without_embeddings.append(predicate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2751b60-36e8-423a-a26a-1f1d39f9cef2",
   "metadata": {},
   "source": [
    "### NER Pipeline\n",
    "\n",
    "We can now combine all these functions to successfully extract both predicates and entities from a question. We will use the model \"...\" (we can still decide on a different model) to recognize entities and proceed in the following way:\n",
    "- Preprocess the question so that special characters that hold no important meaning like ! or : are removed\n",
    "- Extract a list of dictionaries with all the entities from the question using NER\n",
    "    - The dictionaries will look like: {'entity_group' : 'PER', 'word': Andrew Garfield'}\n",
    "- Map the extracted entities to actual nodes in the graph via the editdistance function\n",
    "    - If the distance from the entity in the question and the closest entity in the graph is > 5 then no entity is matched\n",
    "    - If the distance from the entity in the question and the closest entity in the graph is < 5 but > 1 then we prompt the chatbot to ask the user to verify if they matched the right enitity\n",
    "- Remove the entities from the question\n",
    "- Pass the question without entities to the predicate_extraction function\n",
    "- Add the extracted predicates to the list of dictionaries as {'entity_group' : 'predicate', 'word': 'screenwriter'}\n",
    "\n",
    "\n",
    "Some notes on how to handle the 'Entity matching too distant' case. In the final notebook with the speakeasy infrastructure you should make a variable with the matched entities that were too distant. so that they are stored for generating the next message in case they answer 'yes' to the question 'did you mean -matched_entity-?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "49825084-47f4-4957-9c09-854579ad3e10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import re\n",
    "\n",
    "# Create an NER pipeline\n",
    "ner_pipeline = pipeline(\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "8bea2def-2c13-48e7-863a-c20ee803e50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_question(question):\n",
    "    # Remove symbols like :, !, -, etc., by replacing them with an empty string\n",
    "    cleaned_question = re.sub(r'[:!\\\\-]', '', question)\n",
    "    # Remove any extra spaces that might result from removing symbols\n",
    "    cleaned_question = re.sub(r'\\s+', ' ', cleaned_question).strip()\n",
    "    return cleaned_question\n",
    "\n",
    "# Define a function to extract entities and relation from a given question\n",
    "def extract_entities(question, predicate_dict=predicates, n=5, confidence=0.6, max_ngram_size=3):\n",
    "    extracted_entities = []\n",
    "\n",
    "    exit_status = \"\"\n",
    "    question = preprocess_question(question)\n",
    "    print(f\"Question after preprocessing: {question}\\n\")\n",
    "\n",
    "    # Step 1: Use the NER pipeline to get entities in the question\n",
    "    entities = ner_pipeline(question)\n",
    "\n",
    "    # If there are no entities in the question return (maybe prompt the user to double check if the capitalized the right letters)\n",
    "    if entities:\n",
    "    \n",
    "        # Step 2: Turn dictionaries in entities into simplified dictionaries and concatenate words to join_entity['word']\n",
    "        for entity in entities:\n",
    "            simplified_entity = {\n",
    "                'entity_group': entity['entity_group'],\n",
    "                'word': entity['word']\n",
    "            }\n",
    "            extracted_entities.append(simplified_entity)\n",
    "\n",
    "        # Step 3: Remove extracted entities from the question to isolate the predicate phrase\n",
    "        question_no_entities = question\n",
    "        for entity in extracted_entities:\n",
    "            print(f\"Extracted entity: {entity['word']}\\n\")\n",
    "            # Convert both the question and entity to lowercase for consistent replacement\n",
    "            question_no_entities = re.sub(r'\\b' + re.escape(entity['word'].lower()) + r'\\b', '', question_no_entities.lower(), flags=re.IGNORECASE)\n",
    "    \n",
    "        # Replace multiple spaces with a single space and trim leading/trailing whitespace\n",
    "        question_no_entities = re.sub(r'\\s+', ' ', question_no_entities).strip()\n",
    "        \n",
    "        print(f\"Question after removing entities: {question_no_entities}\\n\")\n",
    "\n",
    "        # Step 3.5: Match each entity to the closest node in the graph. Remove them if there is no match\n",
    "        for entity in extracted_entities:\n",
    "    \n",
    "            if match_entity_editdistance(entity['word'], threshold=5):\n",
    "                match_node, match_value, distance = match_entity_editdistance(entity['word'])\n",
    "            \n",
    "                # If the closest entity we can find in the graph is still distant, return the best matched value\n",
    "                # with exit status Entity matching too distant. Then ask the user if the match_value actually \n",
    "                # corresponds to what they wanted\n",
    "                if distance > 3:\n",
    "                    exit_status = 'Entity matching too distant'\n",
    "                    return match_value, exit_status\n",
    "                else:\n",
    "                    # Update 'word' in entity to be the best-matching node's label\n",
    "                    entity['word'] = match_value\n",
    "            else:\n",
    "                # Remove the entity from extracted_entities if no match was found\n",
    "                extracted_entities.remove(entity)\n",
    "\n",
    "    else:\n",
    "        exit_status = 'No entities found by NER'\n",
    "\n",
    "    # Step 4: Extract the relation from the modified question using the extract_relation function\n",
    "    relations = extract_relation(question_no_entities if entities else question, predicates, n=n, confidence=confidence, max_ngram_size=max_ngram_size)\n",
    "\n",
    "    # Step 4.5: Check if the extracted relations have an embedding\n",
    "    for relation in relations:\n",
    "        if relation in pred_without_embeddings:\n",
    "            exit_status = 'predicate missing embedding'\n",
    "            return relation, exit_status\n",
    "                \n",
    "    # Step 5: Add the relation to the extracted_entities list if a match is found\n",
    "    if relations:\n",
    "        print(f\"Extracted predicates: {relations}\\n\")\n",
    "        extracted_entities.append({'entity_group': 'predicate', 'word': []})\n",
    "        for relation in relations:\n",
    "            extracted_entities[-1]['word'].append(relation)\n",
    "\n",
    "    return extracted_entities, exit_status\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "a87816e1-8fa5-4780-90c0-cc0354e8734b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question after preprocessing: Who is the director of the godfather\n",
      "\n",
      "ngram: director godfather\n",
      "lemma: director godfather\n",
      "\n",
      "Extracted predicates: ['director', 'developer']\n",
      "\n",
      "Extracted entities: [{'entity_group': 'predicate', 'word': ['director', 'developer']}]\n",
      "Exit status: No entities found by NER\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Who is the director of the godfather\"\n",
    "extracted_entities = extract_entities(sentence, predicates, n=2, confidence=0.6)\n",
    "print(\"Extracted entities:\", extracted_entities[0])\n",
    "print(\"Exit status:\", extracted_entities[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a019e4d0-cce6-4062-bf72-f35e12c32ab3",
   "metadata": {},
   "source": [
    "### Embeddings\n",
    "\n",
    "Now that we have a reliable way of extracting entities and predicates from the question we can turn them into embeddigs:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9eda7e3-398d-4689-86ca-6cf304f0bcac",
   "metadata": {},
   "source": [
    "ent2id can be used to retrieve the index of an entity in the embedding matrix given it's Uri. Retriving the embedding of an entity given it's label would look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "86bc2f81-0296-479a-81fe-78a1d69a1a54",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The URI of The Godfather is http://www.wikidata.org/entity/Q47703\n",
      "\n",
      "The id of The Godfather is 34515\n",
      "\n",
      "The embedding of The Godfather has lenght 256\n",
      "\n"
     ]
    }
   ],
   "source": [
    "entity_label = 'The Godfather'\n",
    "\n",
    "# Turn label into URI\n",
    "Uri = ent2uri[entity_label]\n",
    "print(f\"The URI of {entity_label} is {Uri}\\n\")\n",
    "\n",
    "# Turn URI into a row index\n",
    "id = ent2id[Uri]\n",
    "print(f\"The id of {entity_label} is {id}\\n\")\n",
    "\n",
    "# Look up the row index in the embedding matrix\n",
    "entity_embedding = entity_matrix[id]\n",
    "print(f\"The embedding of {entity_label} has lenght {len(entity_embedding)}\\n\") # I don't print it cause it's long \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "21633989-8062-46c9-a03a-10cc4bf2d595",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The URI of director is http://www.wikidata.org/prop/direct/P57\n",
      "\n",
      "The id of director is 12\n",
      "\n",
      "The embedding of director has lenght 256\n",
      "\n"
     ]
    }
   ],
   "source": [
    "entity_label = 'director'\n",
    "\n",
    "# Turn label into URI\n",
    "Uri = pred2uri[entity_label]\n",
    "print(f\"The URI of {entity_label} is {Uri}\\n\")\n",
    "\n",
    "# Turn URI into a row index\n",
    "id = pred2id[Uri]\n",
    "print(f\"The id of {entity_label} is {id}\\n\")\n",
    "\n",
    "# Look up the row index in the embedding matrix\n",
    "entity_embedding = predicate_matrix[id]\n",
    "print(f\"The embedding of {entity_label} has lenght {len(entity_embedding)}\\n\") # I don't print it cause it's long "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731a787c-5bce-479a-a271-65ebca1bc815",
   "metadata": {},
   "source": [
    "### Turn labels into embeddings\n",
    "\n",
    "We write a function to make embedding retrival more straightforward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "ca61fa07-eaf2-4826-ae79-d189684e5024",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_embedding(label, type='entity'):\n",
    "\n",
    "    if type=='entity':\n",
    "        pipeline = [ent2uri, ent2id, entity_matrix]\n",
    "    else:\n",
    "        pipeline = [pred2uri, pred2id, predicate_matrix]\n",
    "\n",
    "    \n",
    "    Uri = pipeline[0][label]\n",
    "    \n",
    "    # Turn URI into a row index\n",
    "    id = pipeline[1][Uri]\n",
    "    \n",
    "    # Look up the row index in the embedding matrix\n",
    "    entity_embedding = pipeline[2][id]\n",
    "\n",
    "    return entity_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "3da6cb93-a41b-4d34-9bcd-2e5f73f41bb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The embedding of The Godfather has lenght 256\n",
      "\n",
      "The embedding of director has lenght 256\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "\n",
    "entity_label = 'The Godfather'\n",
    "entity_embedding = extract_embedding(entity_label)\n",
    "print(f\"The embedding of {entity_label} has lenght {len(entity_embedding)}\\n\")\n",
    "\n",
    "pred_label = 'director'\n",
    "pred_embedding = extract_embedding(pred_label, 'predicate')\n",
    "print(f\"The embedding of {pred_label} has lenght {len(pred_embedding)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb61a295-26fa-4036-9520-e8c087986c2d",
   "metadata": {},
   "source": [
    "### Turn embeddings into labels\n",
    "\n",
    "We need also a way to turn an embedding into a label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "031643c4-a994-464f-bde5-367a2dcfd8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_label(embedding, type='entity'):\n",
    "\n",
    "    if type=='entity':\n",
    "        pipeline = [entity_matrix, id2ent, nodes]\n",
    "    else:\n",
    "        pipeline = [predicate_matrix, id2pred, predicates]\n",
    "\n",
    "    # Find the index in the entity embeddings matrix that corresponds to the embedding vector\n",
    "    id = np.where((pipeline[0] == embedding).all(axis=1))[0][0]\n",
    "\n",
    "    # Turn the id into a URI\n",
    "    Uri = pipeline[1][id]\n",
    "\n",
    "    # Turn the URI into a label\n",
    "    label = pipeline[2][Uri]\n",
    "\n",
    "    return label\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "e944b122-afd5-4c7e-80cd-b2dcbbc4cd4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The embedding of The Godfather has lenght 256\n",
      "\n",
      "The extracted label for entity: The Godfather is The Godfather\n",
      "\n",
      "The embedding of characters has lenght 256\n",
      "\n",
      "The extracted label for predicate: characters is characters\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "\n",
    "entity_label = 'The Godfather'\n",
    "entity_embedding = extract_embedding(entity_label)\n",
    "print(f\"The embedding of {entity_label} has lenght {len(entity_embedding)}\\n\")\n",
    "\n",
    "# Turn the embedding back into a label\n",
    "label = extract_label(entity_embedding)\n",
    "print(f\"The extracted label for entity: {entity_label} is {label}\\n\")\n",
    "\n",
    "pred_label = 'characters'\n",
    "pred_embedding = extract_embedding(pred_label, 'predicate')\n",
    "print(f\"The embedding of {pred_label} has lenght {len(pred_embedding)}\\n\")\n",
    "\n",
    "# Turn the embedding back into a label\n",
    "label = extract_label(pred_embedding, 'predicate')\n",
    "print(f\"The extracted label for predicate: {pred_label} is {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8addb9fb-1645-4f32-af3d-71967175491e",
   "metadata": {},
   "source": [
    "### Evaluate embeddings similarity\n",
    "\n",
    "Given the embedding of an entity we want to find the most similar entities in the graph to said entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "eacf5a57-56f9-4b84-a192-6da08c349738",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import pairwise_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "5c543acc-793c-4365-9512-229c6f9162cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_similarities(embedding, n):\n",
    "\n",
    "    embedding = np.atleast_2d(embedding)\n",
    "\n",
    "    answer = []\n",
    "\n",
    "    dist = pairwise_distances(embedding, entity_matrix)\n",
    "    for idx in dist.argsort().reshape(-1)[:n]:\n",
    "        answer.append(nodes[id2ent[idx]])\n",
    "\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "cf43a18f-d845-43f4-81f1-f87c1d9764d6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Batman', 'Deathstroke', 'Harley Quinn', 'The Joker', 'Killer Croc']\n"
     ]
    }
   ],
   "source": [
    "# Example Usage\n",
    "\n",
    "entity_embedding = extract_embedding('Batman')\n",
    "\n",
    "print(find_similarities(entity_embedding, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9625da-eeb8-4b7c-8394-0360f83a77e8",
   "metadata": {},
   "source": [
    "### Answer questions\n",
    "\n",
    "We can now use the following pipeline for answering questions:\n",
    "- Extract the entities and relation from the question\n",
    "- Turn entities and relation into embeddings\n",
    "- If the entity is a subject, retrieve the object by: _object = subject + relation_\n",
    "- If the entity is an object, retrieve the subject by _subject = object - relation_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "607c075b-b1bc-40d3-a1cd-e3a7c64948dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_question_embeddings(question):\n",
    "    entities, exit_status = extract_entities(question, predicates, n=3, confidence=0.6)\n",
    "\n",
    "    # Handle cases based on exit_status\n",
    "    if exit_status == 'No entities found by NER':\n",
    "        return \"We could not find any entities in the question. Could you verify that you have capitalized the right letters, such as movie titles or peopleâs names?\"\n",
    "\n",
    "    elif exit_status == 'Entity matching too distant':\n",
    "        match_value, _ = entities  # entities contains the match value in this case\n",
    "        return f\"The closest entity match found was '{match_value}', but it seems too distant. Could you rephrase it or specify it more clearly?\"\n",
    "\n",
    "    elif exit_status == 'predicate missing embedding':\n",
    "        relation, _ = entities  # entities contains the relation in this case\n",
    "        return f\"Unfortunately, we were not provided with an embedding for the relation '{relation}'. Please try another question.\"\n",
    "\n",
    "    # Proceed if everything worked correctly\n",
    "    if not exit_status:\n",
    "        # Extract predicates and entities\n",
    "        extracted_predicates = [d['word'] for d in entities if d['entity_group'] == 'predicate'][0]\n",
    "        extracted_entities = [d['word'] for d in entities if d['entity_group'] != 'predicate']\n",
    "\n",
    "        # Convert predicates and entities to embeddings\n",
    "        predicates_embeddings = [extract_embedding(pred, 'predicate') for pred in extracted_predicates]\n",
    "        entities_embeddings = [extract_embedding(ent) for ent in extracted_entities]\n",
    "\n",
    "        # Compute answer using similarity function\n",
    "        answer = find_similarities(entities_embeddings[0] + predicates_embeddings[0], 10)\n",
    "        return answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "1251c5d7-4008-429e-a518-3a4ef51645b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question after preprocessing: Who is the director of Star Wars Episode VI Return of the Jedi?\n",
      "\n",
      "Extracted entity: Star Wars Episode VI Return of the Jedi\n",
      "\n",
      "Question after removing entities: who is the director of ?\n",
      "\n",
      "Extracted predicates: ['director']\n",
      "\n",
      "['George Lucas', 'Anthony Daniels', 'Ellis Rubin', 'Lawrence Kasdan', 'Richard Driscoll', 'Mike Quinn', 'Kenny Baker', 'James Kahn', 'Sebastian Shaw', 'Ahmed Best']\n"
     ]
    }
   ],
   "source": [
    "# Example Usage\n",
    "\n",
    "question = \"Who is the director of Star Wars: Episode VI - Return of the Jedi?\"\n",
    "\n",
    "print(answer_question_embeddings(question))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd239b3-664e-4461-9dd9-940b4bb3f463",
   "metadata": {},
   "source": [
    "# TODO\n",
    "\n",
    "- Implement way to handle double questions like \"who is the director of ... AND who is the screenwriter of ....\"\n",
    "- Finish and perfect factual questions queries\n",
    "- Implement language model to generate more realistic responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bfd99b6-b859-4bf3-8a26-8f865cefea45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
